{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deconvolution\n",
    "Convolution Layer는 Convolution 연산을 통해 Feature map의 크기를 줄이고 특정 Feature를 추출하는 역할을 담당하고 있습니다.\n",
    "\n",
    "이와 반대로 Deconvolution은 Feature map의 크기를 증가시키는 방식으로 동작합니다.\n",
    "\n",
    "---------------------\n",
    "* tf.keras.layers.Conv2DTranspose()\n",
    "    * filters : Output filter 개수\n",
    "    * kernel_size : Convolution Kernel의 크기\n",
    "    * padding : same or valid\n",
    "    * strides : Kernel이 움직이는 폭\n",
    "    \n",
    "--------------------------\n",
    "* Deconvolution 동작 방식\n",
    "\n",
    "https://kasausyrzlhe1066469.cdn.ntruss.com/global/file/p/5d3aa68ca555dd7d27452d50/99CA8E3359FE990510.gif\n",
    "\n",
    "Deconvolution은 픽셀 주위에 Zero padding을 통해 feature map의 크기를 우선 늘려줍니다.\n",
    "\n",
    "그 후 늘어난 feature map에 Convolution 연산을 해줍니다.\n",
    "\n",
    "Deconvolution은 CNN의 결과물을 반대로 되돌려 Input과 같은 사이즈를 만들때 사용합니다. 이를 통해 Segmentation이나 CNN Visualization에 사용됩니다.\n",
    "\n",
    "### 실습\n",
    "임의의 Feature map feature_map을 tuple 형태로 선언해주세요.\n",
    "\n",
    "Conv2DTranspose로 Deconvolution layer를 쌓아보세요.  \n",
    "(Conv2DTranspose sample codes https://www.programcreek.com/python/example/93303/keras.layers.convolutional.Conv2DTranspose)\n",
    "\n",
    "Deconv_model의 결과 Shape이 임의의 이미지 image_size와 같아지도록 모델을 구성해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0807 00:12:03.701199 15520 __init__.py:308] Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "W0807 00:12:04.977176 15520 __init__.py:335] Limited tf.summary API due to missing TensorBoard installation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_transpose (Conv2DTran (None, 32, 32, 32)        18464     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 64, 64, 16)        4624      \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 128, 128, 8)       1160      \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 256, 256, 3)       219       \n",
      "=================================================================\n",
      "Total params: 24,467\n",
      "Trainable params: 24,467\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Feature Map을 입력 이미지의 Shape과 똑같이 복원해냈습니다.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2DTranspose, MaxPooling2D, AveragePooling2D, Dropout, Flatten, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "# 입력 이미지의 Shape입니다.\n",
    "image_size = (256, 256, 3)\n",
    "\n",
    "# TODO : 임의의 Feature map (16,16,64)을 만들어줍니다.\n",
    "feature_map = (16,16,64)\n",
    "\n",
    "# Deconvolution Model입니다.\n",
    "Deconv_model = keras.Sequential([\n",
    "# TODO : Conv2DTranspose를 4층 이상 쌓아보세요.\n",
    "keras.layers.Conv2DTranspose(input_shape =feature_map ,filters= 32, kernel_size=(3,3), strides = 2, padding='same'),\n",
    "keras.layers.Conv2DTranspose(filters=16,kernel_size=(3,3),strides = 2, padding='same'),\n",
    "keras.layers.Conv2DTranspose(filters=8,kernel_size=(3,3), strides = 2, padding='same'),\n",
    "\n",
    "# TODO : 마지막 Conv2DTranspose를 통해 입력 이미지의 Shape과 같은 (256,256,3)의 Shape을 만들어주세요.\n",
    "keras.layers.Conv2DTranspose(filters=3,kernel_size = (3,3), strides =2, padding='same')\n",
    "])   #rgb chenal 이 filters 수 = 3\n",
    "# Deconvolution Model 구조 출력\n",
    "Deconv_model.summary()\n",
    "\n",
    "# image_size와 Deconv_model의 결과 Shape 비교\n",
    "if image_size == Deconv_model.layers[-1].output_shape[1:4]:\n",
    "    print('Feature Map을 입력 이미지의 Shape과 똑같이 복원해냈습니다.')\n",
    "else:\n",
    "    print('입력 이미지의 Shape과 Output Feature map의 Shape이 다릅니다.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Visualization\n",
    "CNN에서 Convolution kernel의 효과가 있는지 판단하기 위해 CNN 시각화를 합니다.\n",
    "\n",
    "즉, Convolution Layer에 있는 필터들의 역할을 눈으로 볼수 있게 만드는 것입니다.\n",
    "\n",
    "이번 실습에서는 간단한 CNN을 학습시키고 각 Layer의 결과를 뽑아 시각화를 진행해보겠습니다.\n",
    "\n",
    "* model.layers : model안의 Layer 정보를 return 합니다.\n",
    "\n",
    "* model.layers.output : 각 Layer의 output을 return 합니다.\n",
    "\n",
    "### 실습\n",
    "작성된 코드를 보고 이해해보세요.\n",
    "\n",
    "layer_outputs에 layer의 output을 모두 저장해보세요.\n",
    "\n",
    "display_feature_map()의featuremap_index를 바꿔가며 각 Layer 별 시각화 결과를 확인해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Predict\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "layer_conv1 (Conv2D)         (None, 28, 28, 64)        640       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "maxPool1 (MaxPooling2D)      (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "layer_conv2 (Conv2D)         (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "maxPool2 (MaxPooling2D)      (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv3 (Conv2D)               (None, 7, 7, 32)          18464     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "maxPool3 (MaxPooling2D)      (None, 3, 3, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 288)               0         \n",
      "_________________________________________________________________\n",
      "fc0 (Dense)                  (None, 64)                18496     \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 76,938\n",
      "Trainable params: 76,938\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2000 samples\n",
      "Epoch 1/5\n",
      "2000/2000 - 12s - loss: 0.0871 - accuracy: 0.2445\n",
      "Epoch 2/5\n",
      "2000/2000 - 11s - loss: 0.0568 - accuracy: 0.6075\n",
      "Epoch 3/5\n",
      "2000/2000 - 11s - loss: 0.0336 - accuracy: 0.7640\n",
      "Epoch 4/5\n",
      "2000/2000 - 11s - loss: 0.0198 - accuracy: 0.8650\n",
      "Epoch 5/5\n",
      "2000/2000 - 10s - loss: 0.0131 - accuracy: 0.9145\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, Activation,MaxPooling2D,Dropout ,BatchNormalization, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# 각 Layer의 결과를 시각화하기 위한 함수입니다.\n",
    "def display_feature_map(feature_maps, col_size, row_size, featuremap_index): \n",
    "    activation = feature_maps[featuremap_index]\n",
    "    activation_index=0\n",
    "    fig, ax = plt.subplots(row_size, col_size)\n",
    "    for row in range(0,row_size):\n",
    "        for col in range(0,col_size):\n",
    "            ax[row][col].imshow(activation[0, :, :, activation_index], cmap='gray')\n",
    "            activation_index += 1\n",
    "#     fig.savefig('plot.png')\n",
    "\n",
    "# CNN 모델입니다.\n",
    "def CNN():\n",
    "    Shape=(28,28,1)\n",
    "    inputs = Input(Shape)\n",
    "\n",
    "    x = Conv2D(64,(3,3),strides = (1,1),name='layer_conv1',padding='same')(inputs)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((2,2),name='maxPool1')(x)\n",
    "\n",
    "    x = Conv2D(64,(3,3),strides = (1,1),name='layer_conv2',padding='same')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((2,2),name='maxPool2')(x)\n",
    "\n",
    "    x = Conv2D(32,(3,3),strides = (1,1),name='conv3',padding='same')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((2,2),name='maxPool3')(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(64,activation = 'relu',name='fc0')(x)\n",
    "    x = Dense(32,activation = 'relu',name='fc1')(x)\n",
    "    x = Dense(10,activation = 'softmax',name='fc2')(x)\n",
    "\n",
    "    model = Model(inputs = [inputs],outputs = [x], name='Predict')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# MNIST 데이터 세트를 불러옵니다.\n",
    "mnist = np.load('./data/mnist.npz')\n",
    "# 중간의 Feature map만을 보기위해 Train data만 사용합니다.\n",
    "X_train, y_train= mnist['x_train'][:2000], mnist['y_train'][:2000]\n",
    "\n",
    "\n",
    "# MNIST 데이터 전처리\n",
    "X_train = X_train.astype(np.float32) / 255.\n",
    "X_train = np.expand_dims(X_train, axis=-1)\n",
    "y_train = to_categorical(y_train, 10)\n",
    "\n",
    "# Model을 불러옵니다.\n",
    "model = CNN()\n",
    "model.summary()\n",
    "\n",
    "# Model을 학습합니다.\n",
    "model.compile(optimizer = 'adam', loss = 'mse', metrics = ['accuracy'])\n",
    "model.fit(X_train, y_train, epochs = 5, batch_size = 100, verbose = 2)\n",
    "\n",
    "# TODO : 각 Layer의 Output을 모두 저장합니다.\n",
    "layer_outputs = [layer.output for layer in model.layers]\n",
    "\n",
    "# 각 LAyer별로 Output을 뽑아내기 위해 Model로 만들어줍니다.\n",
    "visualize_model = Model(inputs=model.input, outputs=layer_outputs)\n",
    "\n",
    "# MNIST 이미지 한장을 넣어 모든 Layer의 Feature map의 결과를 뽑아냅니다.\n",
    "feature_maps = visualize_model.predict(X_train[10].reshape(1,28,28,1))\n",
    "\n",
    "# Feature Map을 시각화 합니다.\n",
    "display_feature_map(feature_maps, 5, 5, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 영상 분할 (Image Segmentation)\n",
    "Semantic Segmentation은 컴퓨터 비전 분야에서 많이 연구되어온 분야입니다. 사진에서 물체 (Object)를 찾고 물체와 배경을 분리해내는 작업을 Semantic Segmentation이라고 합니다.\n",
    "\n",
    "Semantic Segmentation은 Classification, Detection 과도 많은 연관이 있으며, 픽셀 단위의 예측을 수행하여 대상을 분리해냅니다.\n",
    "\n",
    "https://kasausyrzlhe1066469.cdn.ntruss.com/global/file/p/5d3abb162052a63e723df2bb/segmentation.JPG\n",
    "\n",
    "이번 실습에서는 간단하게 Fully Convolution Network로 만든 Segmentation model이 어떻게 구성되어있는지 알아보겠습니다.\n",
    "\n",
    "## FCN\n",
    "FCN은 Fully Convolution Network의 약자로 CNN을 기반으로 이루어져 있습니다.\n",
    "\n",
    "https://kasausyrzlhe1066469.cdn.ntruss.com/global/file/p/5d3abbf758509f97fffb9b27/segmentation_architecture.JPG\n",
    "\n",
    "기존의 CNN과 다른점은 모델의 뒤쪽에서 fully connected layer를 사용하지 않고 1 x 1 Convolution layer를 사용하여 Segmentation을 진행합니다.\n",
    "\n",
    "그리고 모델의 마지막 단에서 Upsampling을 통해 줄어든 Feature map의 크기를 다시 키우는 작업을 수행합니다.\n",
    "\n",
    "-----------------------\n",
    "* tf.keras.layers.UpSampling2D()\n",
    "    * size : Upsampling을 위한 factor 값\n",
    "    * interpolation : Upsampling 시 늘어난 feature map 안을 채울 방법을 선택합니다. (nearest or bilinear)\n",
    "    \n",
    "관련 링크\n",
    "\n",
    "https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf  \n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/layers/UpSampling2D  \n",
    "### 실습\n",
    "Segmentation()을 완성해보세요.\n",
    "\n",
    "Segmentation()의 결과 Shape이 임의의 이미지 image_shape와 같아지도록 모델을 구성해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 256, 256, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 128, 128, 64)      1792      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 64, 64, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 32)        18464     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 32)        2080      \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 16, 16, 16)        528       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 16, 16, 3)         51        \n",
      "_________________________________________________________________\n",
      "up_sampling2d (UpSampling2D) (None, 256, 256, 3)       0         \n",
      "=================================================================\n",
      "Total params: 78,339\n",
      "Trainable params: 78,339\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Segmentation을 입력 이미지의 크기와 똑같이 복원해냈습니다.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, UpSampling2D, AveragePooling2D, Dropout, Flatten, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# 임의의 이미지 Shape\n",
    "image_shape = (256,256,3)\n",
    "\n",
    "# 간단한 Segmetation Model입니다.\n",
    "def Segmentation():\n",
    "    shape = (256,256,3)\n",
    "    inputs = Input(shape)\n",
    "    \n",
    "    # TODO : 4 층의 3 x 3 Convolution Layer를 쌓아보세요. (padding = 'same', strides = 2)\n",
    "    conv1 = Conv2D(64,(3,3),strides = 2,padding='same')(inputs)\n",
    "    conv2 = Conv2D(64,(3,3),strides = 2,padding='same')(conv1)\n",
    "    conv3 = Conv2D(32,(3,3),strides = 2,padding='same')(conv2)\n",
    "    conv4 = Conv2D(64,(3,3),strides = 2,padding='same')(conv3)\n",
    "    \n",
    "    # TODO : 3 층의 1 x 1 Convolution Layer를 쌓아보세요. (padding = 'same', strides = 1)\n",
    "    conv5 = Conv2D(32,(1,1),strides = 1,padding='same')(conv4)\n",
    "    conv6 = Conv2D(16,(1,1),strides = 1,padding='same')(conv5)\n",
    "    conv7 = Conv2D(filters=3,kernel_size=(1,1),strides = 1,padding='same',activation = 'relu')(conv6)\n",
    "    \n",
    "    # TODO : Upsampling을 통해 image_shape와 같은 (256,256,3)의 output을 만들어보세요.\n",
    "    upsampling = UpSampling2D(size = (16,16))(conv7)\n",
    "    \n",
    "    # 쌓은 Layer들을 모델로 만들어줍니다.\n",
    "    model = Model(inputs = [inputs], outputs = [upsampling])\n",
    "    \n",
    "    return model\n",
    "    \n",
    "seg_model = Segmentation()\n",
    "seg_model.summary()\n",
    "\n",
    "# image_shape와 seg_model의 결과 Shape 비교\n",
    "if image_shape == seg_model.layers[-1].output_shape[1:4]:\n",
    "    print('Segmentation을 입력 이미지의 크기와 똑같이 복원해냈습니다.')\n",
    "else:\n",
    "    print('입력 이미지의 크기와 Model Output Shape이 다릅니다.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection Case\n",
    "Object Detection은 두 가지 경우가 있습니다.\n",
    "\n",
    "1. Classification과 Localization를 동시에 수행하는 경우\n",
    "\n",
    "2. Localization를 먼저 수행 후 Classification을 수행하는 경우\n",
    "\n",
    "이번 실습에서는 1번 Case의 간단한 Object Detection Model 구조를 확인해보겠습니다.\n",
    "\n",
    "관련 자료 -https://arxiv.org/pdf/1311.2524.pdf\n",
    "\n",
    "### 실습\n",
    "작성된 Object_Detection Class를 보고 이해해보세요.\n",
    "\n",
    "summary()를 통해 모델의 전체 구조를 확인해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.6/mobilenet_2_5_224_tf_no_top.h5\n",
      "2113536/2108140 [==============================] - 10s 5us/step\n",
      "Model: \"object__detection\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 225, 225, 3)  0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 112, 112, 8)  216         conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 112, 112, 8)  32          conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (ReLU)               (None, 112, 112, 8)  0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_1 (DepthwiseConv2D)     (None, 112, 112, 8)  72          conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_1_bn (BatchNormalizatio (None, 112, 112, 8)  32          conv_dw_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_1_relu (ReLU)           (None, 112, 112, 8)  0           conv_dw_1_bn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_1 (Conv2D)              (None, 112, 112, 16) 128         conv_dw_1_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_1_bn (BatchNormalizatio (None, 112, 112, 16) 64          conv_pw_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_1_relu (ReLU)           (None, 112, 112, 16) 0           conv_pw_1_bn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_2 (ZeroPadding2D)      (None, 113, 113, 16) 0           conv_pw_1_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_2 (DepthwiseConv2D)     (None, 56, 56, 16)   144         conv_pad_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_2_bn (BatchNormalizatio (None, 56, 56, 16)   64          conv_dw_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_2_relu (ReLU)           (None, 56, 56, 16)   0           conv_dw_2_bn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_2 (Conv2D)              (None, 56, 56, 32)   512         conv_dw_2_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_2_bn (BatchNormalizatio (None, 56, 56, 32)   128         conv_pw_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_2_relu (ReLU)           (None, 56, 56, 32)   0           conv_pw_2_bn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_3 (DepthwiseConv2D)     (None, 56, 56, 32)   288         conv_pw_2_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_3_bn (BatchNormalizatio (None, 56, 56, 32)   128         conv_dw_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_3_relu (ReLU)           (None, 56, 56, 32)   0           conv_dw_3_bn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_3 (Conv2D)              (None, 56, 56, 32)   1024        conv_dw_3_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_3_bn (BatchNormalizatio (None, 56, 56, 32)   128         conv_pw_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_3_relu (ReLU)           (None, 56, 56, 32)   0           conv_pw_3_bn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_4 (ZeroPadding2D)      (None, 57, 57, 32)   0           conv_pw_3_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_4 (DepthwiseConv2D)     (None, 28, 28, 32)   288         conv_pad_4[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_4_bn (BatchNormalizatio (None, 28, 28, 32)   128         conv_dw_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_4_relu (ReLU)           (None, 28, 28, 32)   0           conv_dw_4_bn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_4 (Conv2D)              (None, 28, 28, 64)   2048        conv_dw_4_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_4_bn (BatchNormalizatio (None, 28, 28, 64)   256         conv_pw_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_4_relu (ReLU)           (None, 28, 28, 64)   0           conv_pw_4_bn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_5 (DepthwiseConv2D)     (None, 28, 28, 64)   576         conv_pw_4_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_5_bn (BatchNormalizatio (None, 28, 28, 64)   256         conv_dw_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_5_relu (ReLU)           (None, 28, 28, 64)   0           conv_dw_5_bn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_5 (Conv2D)              (None, 28, 28, 64)   4096        conv_dw_5_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_5_bn (BatchNormalizatio (None, 28, 28, 64)   256         conv_pw_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_5_relu (ReLU)           (None, 28, 28, 64)   0           conv_pw_5_bn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_6 (ZeroPadding2D)      (None, 29, 29, 64)   0           conv_pw_5_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_6 (DepthwiseConv2D)     (None, 14, 14, 64)   576         conv_pad_6[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_6_bn (BatchNormalizatio (None, 14, 14, 64)   256         conv_dw_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_6_relu (ReLU)           (None, 14, 14, 64)   0           conv_dw_6_bn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_6 (Conv2D)              (None, 14, 14, 128)  8192        conv_dw_6_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_6_bn (BatchNormalizatio (None, 14, 14, 128)  512         conv_pw_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_6_relu (ReLU)           (None, 14, 14, 128)  0           conv_pw_6_bn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_7 (DepthwiseConv2D)     (None, 14, 14, 128)  1152        conv_pw_6_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_7_bn (BatchNormalizatio (None, 14, 14, 128)  512         conv_dw_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_7_relu (ReLU)           (None, 14, 14, 128)  0           conv_dw_7_bn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_7 (Conv2D)              (None, 14, 14, 128)  16384       conv_dw_7_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_7_bn (BatchNormalizatio (None, 14, 14, 128)  512         conv_pw_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_7_relu (ReLU)           (None, 14, 14, 128)  0           conv_pw_7_bn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_8 (DepthwiseConv2D)     (None, 14, 14, 128)  1152        conv_pw_7_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_8_bn (BatchNormalizatio (None, 14, 14, 128)  512         conv_dw_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_8_relu (ReLU)           (None, 14, 14, 128)  0           conv_dw_8_bn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_8 (Conv2D)              (None, 14, 14, 128)  16384       conv_dw_8_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_8_bn (BatchNormalizatio (None, 14, 14, 128)  512         conv_pw_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_8_relu (ReLU)           (None, 14, 14, 128)  0           conv_pw_8_bn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_9 (DepthwiseConv2D)     (None, 14, 14, 128)  1152        conv_pw_8_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_9_bn (BatchNormalizatio (None, 14, 14, 128)  512         conv_dw_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_9_relu (ReLU)           (None, 14, 14, 128)  0           conv_dw_9_bn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_9 (Conv2D)              (None, 14, 14, 128)  16384       conv_dw_9_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_9_bn (BatchNormalizatio (None, 14, 14, 128)  512         conv_pw_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_9_relu (ReLU)           (None, 14, 14, 128)  0           conv_pw_9_bn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_10 (DepthwiseConv2D)    (None, 14, 14, 128)  1152        conv_pw_9_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_10_bn (BatchNormalizati (None, 14, 14, 128)  512         conv_dw_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_10_relu (ReLU)          (None, 14, 14, 128)  0           conv_dw_10_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_10 (Conv2D)             (None, 14, 14, 128)  16384       conv_dw_10_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_10_bn (BatchNormalizati (None, 14, 14, 128)  512         conv_pw_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_10_relu (ReLU)          (None, 14, 14, 128)  0           conv_pw_10_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_11 (DepthwiseConv2D)    (None, 14, 14, 128)  1152        conv_pw_10_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_11_bn (BatchNormalizati (None, 14, 14, 128)  512         conv_dw_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_11_relu (ReLU)          (None, 14, 14, 128)  0           conv_dw_11_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_11 (Conv2D)             (None, 14, 14, 128)  16384       conv_dw_11_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_11_bn (BatchNormalizati (None, 14, 14, 128)  512         conv_pw_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_11_relu (ReLU)          (None, 14, 14, 128)  0           conv_pw_11_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_12 (ZeroPadding2D)     (None, 15, 15, 128)  0           conv_pw_11_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_12 (DepthwiseConv2D)    (None, 7, 7, 128)    1152        conv_pad_12[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_12_bn (BatchNormalizati (None, 7, 7, 128)    512         conv_dw_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_12_relu (ReLU)          (None, 7, 7, 128)    0           conv_dw_12_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_12 (Conv2D)             (None, 7, 7, 256)    32768       conv_dw_12_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_12_bn (BatchNormalizati (None, 7, 7, 256)    1024        conv_pw_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_12_relu (ReLU)          (None, 7, 7, 256)    0           conv_pw_12_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_13 (DepthwiseConv2D)    (None, 7, 7, 256)    2304        conv_pw_12_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_13_bn (BatchNormalizati (None, 7, 7, 256)    1024        conv_dw_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_13_relu (ReLU)          (None, 7, 7, 256)    0           conv_dw_13_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_13 (Conv2D)             (None, 7, 7, 256)    65536       conv_dw_13_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_13_bn (BatchNormalizati (None, 7, 7, 256)    1024        conv_pw_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_13_relu (ReLU)          (None, 7, 7, 256)    0           conv_pw_13_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "region_conv2d_1 (Conv2D)        (None, 7, 7, 9)      20745       conv_pw_13_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "region_batchnorm_1 (BatchNormal (None, 7, 7, 9)      36          region_conv2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "classes_conv2d (Conv2D)         (None, 7, 7, 4)      9220        conv_pw_13_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "region_activation_1 (Activation (None, 7, 7, 9)      0           region_batchnorm_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "classes_batchnorm (BatchNormali (None, 7, 7, 4)      16          classes_conv2d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "region (Conv2D)                 (None, 7, 7, 9)      90          region_activation_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "classes_activation (Activation) (None, 7, 7, 4)      0           classes_batchnorm[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bboxes_flatten (Flatten)        (None, 441)          0           region[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "classes_flatten (Flatten)       (None, 196)          0           classes_activation[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bboxes_dense (Dense)            (None, 196)          86632       bboxes_flatten[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "classes_dense (Dense)           (None, 49)           9653        classes_flatten[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bboxes (Reshape)                (None, 7, 7, 4)      0           bboxes_dense[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "classes (Reshape)               (None, 7, 7, 1)      0           classes_dense[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 344,936\n",
      "Trainable params: 339,438\n",
      "Non-trainable params: 5,498\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, Activation, BatchNormalization, Flatten, concatenate, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "class Object_Detection(Model):\n",
    "    # Keras에 내장되어 있는 CNN 모델을 불러옵니다.\n",
    "    # 객체를 검출하기 이전에 CNN을 통해 Feature를 추출하는 Model입니다.\n",
    "    def cnn(input_width = 224, input_height = 224):\n",
    "        return keras.applications.MobileNet(include_top=False, input_shape=(input_height, input_width, 3), weights='imagenet', alpha=0.25)\n",
    "\n",
    "    # region()은 CNN을 거친 Feature map을 받았을 때 현재 위치에 Object가 있는지 없는지의 여부를 판단하는 Model입니다.\n",
    "    # Regien Proposal Network (RPN)\n",
    "    def region(region_input):\n",
    "        # CNN의 Feature map을 받아 Object가 있는지 없는지를 판단합니다.\n",
    "        # RPN은 sliding window에 3 x 3 convolution을 적용해 input feature map을 256(ZFNet)또는 512(VGG)크기의 feature로 mapping합니다. \n",
    "        x = Conv2D(9, kernel_size = 3, padding='same', name='region_conv2d_1')(region_input)\n",
    "        x = BatchNormalization(name='region_batchnorm_1')(x)\n",
    "        x = Activation('relu', name='region_activation_1')(x)\n",
    "        region = Conv2D(9, kernel_size = 1, activation='sigmoid', name='region')(x)\n",
    "        \n",
    "        # 최종적으로 Region Proposal의 값으로 Bounding Box를 그려줍니다.\n",
    "        # RPN의 결과는 box classification layer (classes)와 box regression layer (bboxes)으로 들어갑니다. \n",
    "        return region\n",
    "    \n",
    "    # Bounding box의 parameter를 찾는 regression을 의미합니다.\n",
    "    def bboxes(region_input):\n",
    "        # region에서 받은 Feature map을 평평하게 만들어\n",
    "        # Bounding box를 그릴 좌표를 regression 합니다.\n",
    "        x = Flatten(name='bboxes_flatten')(region_input)\n",
    "        x = Dense(7 * 7 * 4, name='bboxes_dense')(x)\n",
    "        bboxes = Reshape((7, 7, 4), name='bboxes')(x)\n",
    "        \n",
    "        return bboxes\n",
    "\n",
    "    # Classes()는 영상내의 물체가 어떤 물체인지 분류해주는 Classification Model입니다.\n",
    "    def classes(cnn_input):\n",
    "        \n",
    "        x = Conv2D(4, 3, padding='same', name='classes_conv2d')(cnn_input)\n",
    "        x = BatchNormalization(name='classes_batchnorm')(x)\n",
    "        x = Activation('relu', name='classes_activation')(x)\n",
    "        x = Flatten(name='classes_flatten')(x)\n",
    "        x = Dense(7 * 7 * 1, name='classes_dense', activation='sigmoid')(x)\n",
    "\n",
    "        classes = Reshape((7, 7, 1), name='classes')(x)\n",
    "\n",
    "        return classes\n",
    "\n",
    "    # Object Detection은 여러가지 Loss (region, bboxes, classes)를 한번에 계산하도록 구성되어있습니다. \n",
    "    # 기존의 단순한 CNN보다 복잡하지만 객체를 검출하기 위해서 필요한 부분들이니 꼭 기억하세요. \n",
    "    def complete_model():\n",
    "        \n",
    "        # 위의 각각 다른 4개의 Model을 합쳐 하나의 전체적인 Object Detection Model을 만들어냅니다.\n",
    "        cnn = Object_Detection.cnn()\n",
    "        region = Object_Detection.region(cnn.output)\n",
    "        bboxes = Object_Detection.bboxes(region)\n",
    "        classes = Object_Detection.classes(cnn.output)\n",
    "        \n",
    "        # 하나의 입력 Image에 대해 3가지의 Loss를 계산합니다.\n",
    "        model_inputs = [cnn.input]\n",
    "        model_outputs = [region, bboxes, classes]\n",
    "\n",
    "        return Object_Detection(inputs=model_inputs, outputs=model_outputs)\n",
    "        \n",
    "OD = Object_Detection.complete_model()\n",
    "OD.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style Transfer\n",
    "Style transfer는 2015년에 발표된 내용으로, 논문이 발표된 후에 연구자들에게 큰 반응과 관심을 받은 주제입니다.\n",
    "\n",
    "https://kasausyrzlhe1066469.cdn.ntruss.com/global/file/p/5d3ddad029d7034eeb9a4d17/Style_transfer.JPG\n",
    "\n",
    "Style transfer란 Content image (입력 영상), Style image (적용할 스타일 영상) 이렇게 두 영상을 기반으로 새로운 영상을 재구성하는것입니다.  \n",
    "\n",
    "-----------------------------------------\n",
    "* 영상의 윤곽, 형태 : Content image 기반\n",
    "* 스타일, Texture 정보 : Style image 기반  \n",
    "------------------------------\n",
    "__Style transfer__ 를 적용하기 위해 CNN(Pretrained VGG16)의 내부 Feature map을 추출해 Content Loss와 Style Loss를 계산합니다.\n",
    "\n",
    "그 후 출력 영상이 Content & Style image의 feature map과 유사하게 재구성되도록 최적화합니다.\n",
    "\n",
    "__Style transfer__ 는 Feature map 추출과 추출된 Feature map에 대해 Matrix 연산이 필요합니다. 그러므로 이번 실습에서는 Content Loss와 Style Loss가 각각 어떤 수식에 의해 계산되는지 알아보겠습니다.\n",
    "\n",
    "### Content Loss\n",
    "Content Loss는 특정 Layer의 결과를 뽑고 결과 값들의 차를 제곱합으로 정의합니다.\n",
    "\n",
    "https://kasausyrzlhe1066469.cdn.ntruss.com/global/file/p/5d3dce1737cb569fa82f3468/Content.JPG\n",
    "\n",
    "앞쪽의 Layer에서는 Edge 성분 같은 비교적 단순한 특징을 찾고, 뒤쪽의 Layer에서는 영상에서 사물의 형태 같이 복잡한 특징을 추출하기 때문에 주로 뒤쪽 Layer의 출력 값을 사용합니다.\n",
    "\n",
    "### Style Loss\n",
    "Style Loss는 Layer의 출력 값을 뽑아 Gram Matrix를 만드는 과정이 추가됩니다.\n",
    "\n",
    "https://kasausyrzlhe1066469.cdn.ntruss.com/global/file/p/5d3dce1e7dbff6e2541a7279/Gram.JPG\n",
    "\n",
    "----------------------\n",
    "\n",
    "#### Gram Matrix란?\n",
    "\n",
    "Gram Matrix는 Feature Map을 내적하여 만든 행렬로 Feature map 간의 Correlation 정보를 가지고 있습니다.\n",
    "\n",
    "이 Correlation의 정보가 이미지가 가지고 있는 복합적인 스타일을 표현합니다.\n",
    "\n",
    "---------------------------\n",
    "Style Loss는 Style image의 여러 Layer의 출력 값으로 만든 Gram Matrix와 결과 영상의 같은 Layer들의 출력값으로 만든 Gram Matrix의 차를 제곱하여 Error를 계산합니다.\n",
    "\n",
    "https://kasausyrzlhe1066469.cdn.ntruss.com/global/file/p/5d3dce21ad4a1a6db6d2e31c/Style.JPG\n",
    "\n",
    "Gram Matrix 연산이 추가로 들어가는데다 Style Loss는 여러개의 Featrue map을 사용하기 때문에 굉장히 많은 연산량이 필요합니다.\n",
    "\n",
    "* 관련 링크\n",
    "\n",
    "https://arxiv.org/pdf/1508.06576.pdf\n",
    "### 실습\n",
    "content_loss()를 완성해보세요.\n",
    "\n",
    "gram_matrix()의 입력 input_tensor의 shape을 추출해보세요.\n",
    "\n",
    "gram_matrix()에서 Gram_matrix를 계산하고 Return해주세요.\n",
    "\n",
    "style_loss()를 완성해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1106133\n",
      "4390.954\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, Activation,MaxPooling2D,Dropout ,BatchNormalization, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# 임의의 Feature map을 만들어 Loss를 계산해보겠습니다.\n",
    "\n",
    "# Content\n",
    "# 특정 Layer에서 뽑은 하나의 Feature map을 사용합니다,\n",
    "# Content_image의 Layer 결과값입니다.\n",
    "content_image = tf.random.normal([5,10,10,1])\n",
    "# 생성할 Output Image의 Layer 결과값입니다.\n",
    "content_output= tf.random.normal([5,10,10,1])\n",
    "\n",
    "# Style\n",
    "# 여러 Layer에서 뽑은 Feature map들을 사용합니다.\n",
    "# 임의로 3개의 Feature map을 추출했다 가정하겠습니다.\n",
    "# Style Image의 Layer 결과 값입니다.\n",
    "style1 = tf.random.normal([5,5,2])\n",
    "style2 = tf.random.normal([10,10,2])\n",
    "style3 = tf.random.normal([15,15,2])\n",
    "\n",
    "# 생성할 Output Image의 Layer 결과값입니다.\n",
    "style1_output = tf.random.normal([5,5,2])\n",
    "style2_output = tf.random.normal([10,10,2])\n",
    "style3_output = tf.random.normal([15,15,2])\n",
    "\n",
    "\n",
    "# Content Loss를 계산하는 메서드 입니다.\n",
    "def content_loss(content, target):\n",
    "    # Feature map간의 차이를 제곱하여 Loss를 계산합니다.\n",
    "    # TODO : tf.reduce_mean과 tf.squere를 이용하여 Content loss를 만들어보세요. (Hint. MSE)\n",
    "    content_loss = tf.reduce_mean(tf.square(content-target))\n",
    "    \n",
    "    # content_loss= sum((target-content)**2)\n",
    "    \n",
    "    return content_loss\n",
    "\n",
    "# Gram_matrix를 연산하는 메서드입니다.\n",
    "def gram_matrix(input_tensor):\n",
    "    # TODO : 입력 Feature Map의 Width, Height, Channel을 추출해보세요. (Hint. tf.shape)\n",
    "    w, h, c =  input_tensor.shape\n",
    "    \n",
    "    # (C * H * W)의 3차원 형태의 Feature map을 1차원으로 Flatten하게 만들어줍니다.\n",
    "    # Flatten을 통해 연산 속도를 조금 빠르게 만들 수 있습니다.\n",
    "    input_tensor = tf.reshape(input_tensor, [-1, w * h * c])\n",
    "    \n",
    "    # TODO : Gram Matrix를 연산해보세요. (Hint. Transpose)\n",
    "    return  tf.matmul(input_tensor, tf.transpose(input_tensor))\n",
    "\n",
    "\n",
    "# Style Loss를 계산하는 메서드입니다.\n",
    "def style_loss(style, target):\n",
    "    loss = 0\n",
    "    # 추출한 여러개의 Feature map을 Gram Matrix로 만들어줍니다.\n",
    "    # 그 후 Gram Matrix의 차이를 계산한 후 Loss에 더해줍니다.\n",
    "    for i in range(len(style)):\n",
    "        # TODO : 각 Feature map에 대해 Gram matrix를 계산해주세요.\n",
    "        gram_style = gram_matrix(style[i])\n",
    "        gram_target = gram_matrix(target[i])\n",
    "        \n",
    "        # TODO : tf.reduce_mean과 tf.squere를 이용하여 Style loss를 만들어보세요. (Hint. MSE)\n",
    "        # Style Loss는 여러개의 Feature map을 사용하기 때문에 Loss의 값도 여러개가 나옵니다.\n",
    "        # 이를 모두 합쳐서 계산합니다.\n",
    "        loss += tf.reduce_mean(tf.square(gram_style-gram_target))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# Content Loss 계산\n",
    "content_loss= content_loss(content_image, content_output)\n",
    "\n",
    "# Stlye Loss 계산\n",
    "style = [style1, style2, style3]\n",
    "style_output =[style1_output, style2_output, style3_output]\n",
    "style_loss = style_loss(style, style_output)\n",
    "\n",
    "# 아래의 Loss 값은 임의의 Feature map으로 계산한 Loss 때문에 초기의 학습되지 않은 Loss를 의미합니다.\n",
    "# 실제로 모델에 Loss를 적용하여 학습하게 되면 아래의 Loss값들이 점점 작아지는 방향으로 학습을 진행합니다.\n",
    "print(content_loss.numpy())\n",
    "print(style_loss.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Custom Loss Function 설계하기 **\n",
    "19장 실습을 진행하며 각 분야(Object Detection, Segmentation…)의 CNN 구조를 살펴봤습니다. CNN의 구조는 특정 Layer를 제외하고는 어느정도 유사한 것을 볼 수 있습니다.\n",
    "\n",
    "원하는 어플리케이션을 구현하기 위해서는 적절한 CNN을 만드는 것 이외에 적절한 Loss Function의 설계도 필요합니다.\n",
    "\n",
    "앞서 실습에서 CNN을 학습하는데 있어 여러개의 Loss Function을 학습해야 하는 경우가 있습니다.\n",
    "\n",
    "이번 미션을 통해 직접 Loss fucntion을 구현하고 여러 Loss Fucntion을 동시에 최소화하는 CNN을 구현해보겠습니다.\n",
    "\n",
    "### 미션\n",
    "MNIST 데이터를 전처리해보세요.\n",
    "* Float 변환, 0 ~ 1로 Normalize\n",
    "* CNN에 넣기 위한 Dimension 확장\n",
    "* Lable One hot Encoding  \n",
    "\n",
    "custom_loss()를 완성해보세요. 설계할 Loss fucntion은 아래의 두 가지 입니다.\n",
    "\n",
    "* Mean Square Error mse = \\sum(y_{true} - y_{pred})^2mse=∑(ytrue​−ypred​)2\n",
    "\n",
    "* Mean Absolute Error mae = \\sum|y_{true} - y_{pred}|mae=∑∣ytrue​−ypred​∣\n",
    "\n",
    "MNIST 데이터를 학습할 CNN() 구현해보세요.\n",
    "\n",
    "CNN()을 학습시켜주세요.\n",
    "\n",
    "test_accuracy를 96% 이상 나오도록 모델을 설계해보세요.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 14, 14, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 14, 14, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 7, 7, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 4, 4, 32)          18464     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 4, 4, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                16416     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 55,594\n",
      "Trainable params: 55,338\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/9\n",
      "10000/10000 - 23s - loss: 0.1037 - accuracy: 0.7427 - val_loss: 0.2285 - val_accuracy: 0.5280\n",
      "Epoch 2/9\n",
      "10000/10000 - 19s - loss: 0.0209 - accuracy: 0.9541 - val_loss: 0.1470 - val_accuracy: 0.8060\n",
      "Epoch 3/9\n",
      "10000/10000 - 19s - loss: 0.0129 - accuracy: 0.9712 - val_loss: 0.0775 - val_accuracy: 0.8790\n",
      "Epoch 4/9\n",
      "10000/10000 - 18s - loss: 0.0087 - accuracy: 0.9808 - val_loss: 0.0368 - val_accuracy: 0.9380\n",
      "Epoch 5/9\n",
      "10000/10000 - 18s - loss: 0.0066 - accuracy: 0.9861 - val_loss: 0.0242 - val_accuracy: 0.9520\n",
      "Epoch 6/9\n",
      "10000/10000 - 18s - loss: 0.0051 - accuracy: 0.9893 - val_loss: 0.0133 - val_accuracy: 0.9690\n",
      "Epoch 7/9\n",
      "10000/10000 - 18s - loss: 0.0043 - accuracy: 0.9913 - val_loss: 0.0160 - val_accuracy: 0.9600\n",
      "Epoch 8/9\n",
      "10000/10000 - 18s - loss: 0.0037 - accuracy: 0.9924 - val_loss: 0.0114 - val_accuracy: 0.9720\n",
      "Epoch 9/9\n",
      "10000/10000 - 18s - loss: 0.0031 - accuracy: 0.9938 - val_loss: 0.0112 - val_accuracy: 0.9680\n",
      "1000/1000 [==============================] - 1s 802us/sample - loss: 0.0112 - accuracy: 0.9680 - loss: 0.0105 - accuracy: 0.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, Conv2DTranspose, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# MNIST 데이터 세트를 불러옵니다.\n",
    "mnist = np.load('./data/mnist.npz')\n",
    "\n",
    "# TODO : Train과 Test 데이터로 나누어주세요.\n",
    "# 원하는 만큼 데이터를 사용해보세요.\n",
    "X_train, y_train, X_test, y_test = mnist['x_train'][:10000], mnist['y_train'][:10000],mnist['x_test'][:1000], mnist['y_test'][:1000]\n",
    "\n",
    "# TODO : MNIST 데이터를 전처리해주세요.\n",
    "# TODO : 0 ~ 1 Float으로 normalize 해주세요.\n",
    "X_train = X_train.astype(np.float32) / 255.\n",
    "X_test = X_test.astype(np.float32) / 255.\n",
    "\n",
    "# TODO : (num, 28, 28) -> (num, 28, 28 ,1)로 Channel Dimension을 추가해주세요.\n",
    "X_train = np.expand_dims(X_train, axis=-1)\n",
    "X_test = np.expand_dims(X_test, axis=-1)\n",
    "\n",
    "# TODO : Label을 One Hot Encoding 해주세요.\n",
    "y_train =  to_categorical(y_train, 10)\n",
    "y_test =  to_categorical(y_test, 10)\n",
    "\n",
    " # custom_loss를 완성해보세요.\n",
    "def custom_loss(y_true, y_pred):\n",
    "    # TODO : MSE를 구현해주세요.\n",
    "    mse_loss = (tf.square(y_true-y_pred))    \n",
    "    # TODO : MAE를 구현해주세요.\n",
    "    mae_loss = (np.absolute(y_true-y_pred)) \n",
    "    \n",
    "    # 여러개의 Loss를 사용하면 아래와 같이 더해져서 Loss를 계산합니다.\n",
    "    # 일반적으로 여러개의 Loss를 계산할 때는 \n",
    "    # (a * mse_loss) + (b * mae_loss) \n",
    "    # 형태로 각 Loss를 얼마나 반영할 것인가에 대한 Parameter를 사용하는 경우도 많습니다.\n",
    "    return (1*mse_loss) + (1*mae_loss)\n",
    "# TODO : 적절한 CNN을 만들어주세요.\n",
    "def CNN():\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "    # TODO : 3개의 Convolution Layer, 3개의 BatchNormalization을 추가해주세요.\n",
    "    keras.layers.Conv2D(32 ,kernel_size = (3,3), strides = (2,2), padding = 'same', input_shape=(28,28,1)),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('relu'),\n",
    "    \n",
    "    keras.layers.Conv2D(64 ,kernel_size = (3,3), strides = (2,2), padding = 'same'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('relu'),\n",
    "    \n",
    "    keras.layers.Conv2D(32 ,kernel_size = (3,3), strides = (2,2), padding = 'same'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('relu'),\n",
    "    \n",
    "    \n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(32, activation = tf.nn.relu),\n",
    "    keras.layers.Dense(32, activation = tf.nn.relu),\n",
    "    keras.layers.Dense(10, activation = tf.nn.softmax)\n",
    "    ])\n",
    "    return model\n",
    "# TODO : 만든 CNN을 불러와주세요.\n",
    "model = CNN()\n",
    "model.summary()\n",
    "\n",
    "# TODO : Comfile과 fit으로 CNN을 학습시켜주세요. Loss는 직접 설계한 custum_loss를 사용합니다.\n",
    "model.compile(optimizer = 'adam', loss = custom_loss, metrics = ['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs = 9, batch_size = 100, validation_data = (X_test, y_test), verbose = 2)\n",
    "\n",
    "# TODO : CNN 모델을 평가해주세요.\n",
    "# test_accuracy가 96% 이상 나와야합니다.\n",
    "loss, test_accuracy = model.evaluate(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
