{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 간단한 이진트리 분류기 만들기\n",
    "사이킷런을 이용해 의사결정트리 실습을 진행하기 전에 간단한 트리 구조의 분류기를 만들어보겠습니다.\n",
    "\n",
    "직업, 성별, 키 데이터를 기반으로 누구인지를 분류해보겠습니다.\n",
    "\n",
    "- job, height, sex : 분류를 위한 Feature\n",
    "- name : 클래스를 의미\n",
    "\n",
    "https://kasausyrzlhe1066469.cdn.ntruss.com/global/file/p/5d22fb2cfa55a67a67dbcfcc/%EC%9D%B4%EC%A7%84%ED%8A%B8%EB%A6%AC.png\n",
    "\n",
    "위의 그림과 같은 트리구조를 만들어 데이터를 분류해보겠습니다.\n",
    "\n",
    "### 실습\n",
    "Sex_Node()에서 성별 데이터에 따라 male, female 리스트에 idx를 저장해주세요.\n",
    "\n",
    "인덱스 저장 후에 Job_Node()에 인자를 넣어 다음 Node를 호출해주세요.\n",
    "\n",
    "Job_Node()에서 가수인 경우와 배우인 경우를 분류하고 분류가 끝난 데이터는 이름을 출력해주세요.\n",
    "\n",
    "Job_Node()에서 분류가 끝나면 다음 Node인 Height_Node()를 호출해주세요.\n",
    "\n",
    "Height_Node()에서 우선 성별을 나누고 키에 따라 이름을 분류해보세요.\n",
    "\n",
    "트리가 어떻게 구성되어 있는지 순차적으로 코드를 따라가며 이해해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    이름  직업    키 성별\n",
      "0   하하  가수  171  M\n",
      "1  김범수  가수  182  M\n",
      "2   다현  가수  158  F\n",
      "3  아이유  가수  160  F\n",
      "4  최민식  배우  177  M\n",
      "5  김혜수  배우  170  F \n",
      "\n",
      "Node_num : 1 | Node Depth : 1 | Sex_Node\n",
      "남자 Index :  [0, 1, 4]\n",
      "여자 Index :  [2, 3, 5]\n",
      "Node_num : 2 | Node Depth : 2 | Job_Node\n",
      "Node_num : 3 | Node Depth : 2 | Name : 최민식\n",
      "가수 Index :  [0, 1]\n",
      "Node_num : 4 | Node Depth : 3 | Height_Node\n",
      "Node_num : 5 | Node Depth : 3 | Name : 하하\n",
      "Node_num : 6 | Node Depth : 3 | Name : 김범수\n",
      "Node_num : 7 | Node Depth : 2 | Job_Node\n",
      "Node_num : 8 | Node Depth : 2 | Name : 김혜수\n",
      "가수 Index :  [2, 3]\n",
      "Node_num : 9 | Node Depth : 3 | Height_Node\n",
      "Node_num : 10 | Node Depth : 3 | Name : 다현\n",
      "Node_num : 11 | Node Depth : 3 | Name : 아이유\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 직업, 키, 성별로 이름을 구분하는 간단한 이진트리 분류기를 만들어보겠습니다.\n",
    "name = ['하하', '김범수', '다현', '아이유', '최민식', '김혜수']\n",
    "job  = ['가수', '가수'  , '가수', '가수'  , '배우'  , '배우']\n",
    "height = [171, 182, 158, 160, 177, 170]\n",
    "sex = ['M', 'M', 'F', 'F', 'M', 'F']\n",
    "\n",
    "# Node 번호를 지정해주기 위한 변수\n",
    "num = 0\n",
    "\n",
    "# 데이터 프레임 만들기\n",
    "data = pd.DataFrame({'이름': name, '직업': job, '키': height,'성별': sex})\n",
    "print(data,'\\n')\n",
    "\n",
    "# 키 분류 Node\n",
    "def Height_Node(df, idx, depth):\n",
    "    # 전역 변수를 함수 내에서 사용하기 위해 Global 선언\n",
    "    global num\n",
    "    num +=1\n",
    "    # Node num, Depth, Node Name 출력\n",
    "    print('Node_num : {} | Node Depth : {} | Height_Node'.format(num, depth))\n",
    "    \n",
    "    for i in idx:\n",
    "        num +=1\n",
    "        # 성별에 따라 키의 기준이 다르기 때문에 성별을 우선 분류\n",
    "        if df['성별'][i] =='M':\n",
    "            # 남자의 경우 키에 따라 분류\n",
    "            # 키가 180보다 작은 경우\n",
    "            if df.iloc[i].loc['키'] <180:\n",
    "                print('Node_num : {} | Node Depth : {} | Name : {}'.format(num, depth, df['이름'][i]))\n",
    "            # 키가 180보다 큰 경우\n",
    "            else:\n",
    "                print('Node_num : {} | Node Depth : {} | Name : {}'.format(num, depth, df.iloc[i].loc['이름']))\n",
    "        else:\n",
    "            # 여자의 경우 키에 따라 분류\n",
    "            # 키가 160보다 작은 경우\n",
    "            if df.iloc[i].loc['키']<160:\n",
    "                print('Node_num : {} | Node Depth : {} | Name : {}'.format(num, depth, df.iloc[i].loc['이름']))\n",
    "            # 키가 160보다 큰 경우\n",
    "            else:\n",
    "                print('Node_num : {} | Node Depth : {} | Name : {}'.format(num, depth, df.iloc[i].loc['이름']))\n",
    "\n",
    "# 직업 분류 Node\n",
    "def Job_Node(df,idx, depth):\n",
    "    # 전역 변수를 함수 내에서 사용하기 위해 Global 선언\n",
    "    global num\n",
    "    num +=1\n",
    "    \n",
    "    # Node num, Depth, Node Name 출력\n",
    "    print('Node_num : {} | Node Depth : {} | Job_Node'.format(num, depth))\n",
    "    \n",
    "    # Index 저장을 위한 리스트 \n",
    "    singer = []\n",
    "    \n",
    "    for i in idx:\n",
    "        \n",
    "        # 가수인 경우 Index 저장\n",
    "        if df.iloc[i].loc['직업'] == '가수':\n",
    "            singer.append(i)\n",
    "        # 배우인 경우 Node 번호와 해당 배우의 이름 출력    \n",
    "        else:\n",
    "            num += 1\n",
    "            print('Node_num : {} | Node Depth : {} | Name : {}'.format(num , depth, df.iloc[i].loc['이름']))\n",
    "    \n",
    "    # 가수인 경우 분류가 끝나지 않았으므로 Index 출력\n",
    "    print('가수 Index : ',singer)\n",
    "    \n",
    "    # 마지막 분류 기준인 키를 통해 가수를 분류\n",
    "    # 다음 Node를 호출할 때 depth를 하나 증가시켜줍니다.\n",
    "    Height_Node(df, singer, depth+1)\n",
    "\n",
    "# 성별 분류 Node\n",
    "def Sex_Node(df, depth):\n",
    "    # 전역 변수를 함수 내에서 사용하기 위해 Global 선언\n",
    "    global num\n",
    "    # Node num, Depth, Node Name 출력\n",
    "    num +=1\n",
    "    print('Node_num : {} | Node Depth : {} | Sex_Node'.format(num, depth))\n",
    "    \n",
    "    male = []\n",
    "    female = []\n",
    "    # 처음 성별 데이터 전체로 분류\n",
    "    for idx, sex in enumerate(df['성별']):\n",
    "        # 남자인 경우 Index 저장\n",
    "        if sex == 'M':\n",
    "            male.append(idx)\n",
    "        # 여자인 경우 Index 저장\n",
    "        elif sex == 'F':\n",
    "            female.append(idx)\n",
    "    \n",
    "    # Index 확인\n",
    "    print('남자 Index : ',male)\n",
    "    print('여자 Index : ',female)\n",
    "    \n",
    "    # 성별 분류 후 직업을 분류하는 Node를 호출합니다.\n",
    "    # 다음 Node를 호출할 때 depth를 하나 증가시켜줍니다.\n",
    "    Job_Node( df, male, depth+1)\n",
    "    Job_Node( df, female, depth+1 )\n",
    "\n",
    "# 첫 번째 분류 기준으로 성별을 설정합니다.\n",
    "Sex_Node(data, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 의사 결정 트리(DecisionTreeClassifier)\n",
    "여러 가지 규칙을 순차적으로 적용하면서 독립 변수 공간을 분할하는 분류 모형입니다.\n",
    "\n",
    "분류(classification)와 회귀 분석(regression)에 모두 사용될 수 있기 때문에 CART(Classification And Regression Tree)라고도 합니다.\n",
    "\n",
    "분류 실습을 위해 Iris 데이터를 사용하겠습니다. Iris 데이터는 꽃받침 길이, 꽃받침 넓이, 꽃잎 길이, 꽃잎 넓이 네 가지 Feature와 세 종류의 붓꽃 라벨로 구성되어있습니다.\n",
    "\n",
    "각각의 Feature를 의사결정트리에 넣어 어떤 종류의 붓꽃인지 분류해보겠습니다.\n",
    "\n",
    "--------------------------\n",
    "#### DecisionTreeClassifier()\n",
    "* max_depth : 트리의 층\n",
    "* min_samples_split : 각 트리 별 노드의 개수\n",
    "\n",
    "https://kasausyrzlhe1066469.cdn.ntruss.com/global/file/p/5d1aac943b22bd516d794622/%EC%9D%98%EC%82%AC%EA%B2%B0%EC%A0%95%ED%8A%B8%EB%A6%AC.png\n",
    "### 실습\n",
    "load_iris()로 데이터 세트를 불러오세요.\n",
    "\n",
    "train_test_split()으로 학습 데이터와 검증데이터로 나눠보세요.\n",
    "\n",
    "dtree에 의사결정트리 객체를 불러오세요.\n",
    "\n",
    "fit()을 사용해 트리를 학습시켜보세요.\n",
    "\n",
    "검증 데이터로 결과를 예측하고 정확도를 계산해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터 정확도 : 0.9667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def main():\n",
    "    # Iris 데이터 세트 불러오기\n",
    "    iris = load_iris()\n",
    "    # Iris 데이터 세트 Train과 Test 분할하기\n",
    "    # Train : Test = 8 : 2\n",
    "    X_train, X_test, y_train, y_test = train_test_split(iris.data,iris.target, test_size = 0.2, random_state = 121)\n",
    "    # 의사 결정 트리 불러오기\n",
    "    dtree = DecisionTreeClassifier()#max_depth=2,min_samples_split=3)\n",
    "    \n",
    "    # 의사 결정 트리 학습\n",
    "    dtree.fit(X_train,y_train)\n",
    "    \n",
    "    # 검증 데이터로 결과 예측\n",
    "    pred = dtree.predict(X_test)\n",
    "    print('검증 데이터 정확도 : {0:.4f}'.format(accuracy_score(y_test, pred)))\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 최적의 파라미터 찾기\n",
    "### GridSearceCV\n",
    "사이킷런에서 제공하는 API로 교차 검증과 하이퍼 파라미터 튜닝을 동시에 할 수 있습니다.\n",
    "\n",
    "일반적으로 Gridsearch를 이용해 최적의 하이퍼 파라미터 튜닝을 수행한 후에 Test data로 모델을 평가하는 것이 일반적인 머신러닝 모델 적용방법 입니다.\n",
    "\n",
    "이전 실습에서 진행했던 의사결정트리 예제에 GridSearchCV()를 적용해보겠습니다.\n",
    "\n",
    "의사결정트리 내부에는 트리의 깊이와 각 트리당 노드의 개수를 결정하는 파라미터가 있습니다.\n",
    "\n",
    "--------------------\n",
    "- max_depth\n",
    "- min_sample_splits\n",
    "\n",
    "위의 두개의 파라미터에 GridSearchCV()를 적용해 최적의 성능을 내는 깊이와 노드 개수를 찾아보겠습니다.\n",
    "\n",
    "* GridSearchCV()\n",
    "    - estimator : Classifier, Regressor, pipeline 등이 사용됩니다.\n",
    "    - param_gird : Dictionary 값이 주어집니다. estimator의 튜닝을 위해 파라미터 명과 값을 지정합니다.\n",
    "    - scoring : 예측 성능을 측정할 평가 방법을 지정합니다.\n",
    "    - cv : 교차 검증을 위해 분할되는 학습/테스트 개수를 지정합니다.\n",
    "    - refit : 가장 최적의 하이퍼 파라미터를 찾은 후 estimator 객체를 해당 하이퍼 파라미터로 재학습시킵니다. (default = True)\n",
    "    - GridSearchCV() 속성\n",
    "    - best_params_ : 최적의 파라미터 반환\n",
    "    - best_score_ : 최고 정확도 반환\n",
    "    - best_estimator_ : 최적의 파라미터로 학습된 estimator 반환\n",
    "    \n",
    "### 실습\n",
    "작성된 코드를 보며 이해해보세요.\n",
    "\n",
    "iris_data()와 DecisionTreeClassifier() 를 불러오세요.\n",
    "\n",
    "param_grid에 max_depth와 min_samples_split의 값을 딕셔너리 형태로 넣어주세요. 값은 1차원 리스트로 넣어주세요.\n",
    "\n",
    "GridSearchCV()에 dtree, param_grid, cv를 설정해주세요.\n",
    "\n",
    "의사결정트리를 학습시키고 결과를 예측해보세요.\n",
    "\n",
    "데이터 준비, 모델 설정, 학습, 결과 출력이 어떻게 이루어지는지 숙지하세요.\n",
    "\n",
    "param_grid를 바꿔가며 가장 높은 최고 정확도가 나오도록 해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV 최적 파라미터 :  {'max_depth': 3, 'min_samples_split': 2}\n",
      "GridSearchCV 최고 정확도 : 0.9732\n",
      "Test Dataset accuracy : 0.9474\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def main():\n",
    "    # Iris 데이터 세트 불러오기\n",
    "    iris = load_iris()\n",
    "    # Iris 데이터 세트 Train과 Test 분할하기\n",
    "    X_train, X_test, y_train,y_test = train_test_split(iris.data,iris.target)\n",
    "    \n",
    "    # 의사 결정 트리 불러오기\n",
    "    dtree = DecisionTreeClassifier()\n",
    "    \n",
    "    # 의사 결정 트리의 인자 값 설정\n",
    "    # 학습을 진행할 때 트리의 깊이와 각 트리당 노드의 개수를 정해줍니다. \n",
    "    param_grid = {'max_depth' : [1,2,3], 'min_samples_split' : [2,3]}\n",
    "    \n",
    "    # 이러면 총 6가지 경우에 대해 학습을 진행하며 최종적으로는 GridSearchCV가 가장 성능이 좋은 파라미터를 선택합니다.\n",
    "    grid_dtree = GridSearchCV(dtree, param_grid = param_grid, cv =3 , refit= True, return_train_score=True) \n",
    "    '''\n",
    "    GridSearchCV()\n",
    "estimator : Classifier, Regressor, pipeline 등이 사용됩니다.\n",
    "param_gird : Dictionary 값이 주어집니다. estimator의 튜닝을 위해 파라미터 명과 값을 지정합니다.\n",
    "scoring : 예측 성능을 측정할 평가 방법을 지정합니다.\n",
    "cv : 교차 검증을 위해 분할되는 학습/테스트 개수를 지정합니다.\n",
    "refit : 가장 최적의 하이퍼 파라미터를 찾은 후 estimator 객체를 해당 하이퍼 파라미터로 재학습시킵니다. (default = True)\n",
    "'''    \n",
    "    # 의사 결정 트리 학습\n",
    "    grid_dtree.fit(X_train,y_train)\n",
    "    \n",
    "    # 출력을 위한 데이터 프레임 만들기\n",
    "    scores_df = pd.DataFrame(grid_dtree.cv_results_)\n",
    "    \n",
    "    '''\n",
    "    GridSearchCV() 속성\n",
    "best_params_ : 최적의 파라미터 반환\n",
    "best_score_ : 최고 정확도 반환\n",
    "best_estimator_ : 최적의 파라미터로 학습된 estimator 반환\n",
    "\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    scores_df[['params', 'mean_test_score', 'rank_test_score', 'split0_test_score', 'split1_test_score', 'split2_test_score']]\n",
    "    \n",
    "    # GridSearchCV()가 가지고 있는 속성들을 불러 결과를 출력해보세요.\n",
    "    print('GridSearchCV 최적 파라미터 : ', grid_dtree.best_params_)\n",
    "    print('GridSearchCV 최고 정확도 : {0:.4f}'.format(grid_dtree.best_score_ ))\n",
    "    \n",
    "    # 최적의 파라미터로 학습된 트리 Estimator 가져오세요.\n",
    "    estimator = grid_dtree.best_estimator_\n",
    "    \n",
    "    # 검증 데이터로 결과 예측\n",
    "    pred = estimator.predict(X_test)\n",
    "    print('Test Dataset accuracy : {0:.4f}'.format(accuracy_score(y_test, pred)))\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 앙상블(Ensemble) 학습 (1)\n",
    "앙상블 학습은 여러 개의 분류기 (Classifier)를 설계하고 각 분류기 별 예측을 결합하여 보다 정확한 결과를 도출하는 기법을 말합니다.\n",
    "\n",
    "앙상블 학습 유형으로는 Voting, Bagging, Boosting 등 여러 유형이 있습니다.\n",
    "\n",
    "이번 예제는 Voting에 대해 진행해보겠습니다.\n",
    "\n",
    "### Voting\n",
    "Voting은 의미 그대로 투표를 통해 값을 결정하는 것입니다. Voting은 Hard voting과 Soft voting이 있습니다.\n",
    "\n",
    "- Hard Voting : 다수의 분류기가 예측한 값을 최종 값으로 선택\n",
    "- Soft Voting : 각 레이블 별로 예측 확률을 낸 후 평균을 내어 최종 값으로 선택\n",
    "\n",
    "일반적으로 Soft voting이 성능이 더 좋아 더 많이 사용됩니다.\n",
    "\n",
    "- VotingClassifier(estimators, voting)\n",
    "    - estimators : 앙상블 학습을 진행할 다른 분류기 설정\n",
    "    - voting : voting 방식 선택\n",
    "\n",
    "https://kasausyrzlhe1066469.cdn.ntruss.com/global/file/p/5d1ab3bed8c0611a5617d406/Voting.png\n",
    "\n",
    "### 실습\n",
    "유방암 데이터를 불러오세요.\n",
    "\n",
    "종류가 다른 분류기 LogisticRegression()와 KNeighborsClassifier()를 불러오세요.\n",
    "\n",
    "VotingClassifier()를 설정하세요.\n",
    "\n",
    "- estimator : [(‘LR’, lr_clf),(‘KNN’,knn_clf)]\n",
    "- voting : soft 설정\n",
    "\n",
    "데이터를 나누고 VotingClassifier()를 학습시켜 정확도를 예측해보세요.\n",
    "\n",
    "다른 분류기를 개별로 사용했을 때 결과를 출력해보세요.\n",
    "\n",
    "앙상블 학습을 했을 때와 그렇지 않을 때의 결과가 어떻게 다른지 확인해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier 정확도 : 0.9441\n",
      "LogisticRegression 정확도 : 0.9510\n",
      "KNeighborsClassifier 정확도 : 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 유방암 데이터 불러오기\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "# 데이터 프레임을 만들기\n",
    "data_df = pd.DataFrame(cancer.data, columns = cancer.feature_names)\n",
    "\n",
    "# Voting과 비교할 각각 다른 분류기 불러오기\n",
    "# KNeighborsClassifier, LogisticRegression\n",
    "lr_clf = LogisticRegression()\n",
    "knn_clf = KNeighborsClassifier(n_neighbors = 8)\n",
    "\n",
    "\n",
    "# Voting에 사용할 분류기\n",
    "# 분류기로 LogisticRegressor와 KNeighborClassifier를 사용합니다.\n",
    "# Voting 방식은 Soft Voting을 사용합니다.\n",
    "\n",
    "vo_clf = VotingClassifier( estimators = [('LR', lr_clf),('KNN',knn_clf)], voting ='soft' )\n",
    "'''\n",
    "Hard Voting : 다수의 분류기가 예측한 값을 최종 값으로 선택\n",
    "Soft Voting : 각 레이블 별로 예측 확률을 낸 후 평균을 내어 최종 값으로 선택\n",
    "'''\n",
    "# 학습 데이터와 검증 데이터로 나누기\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_df,cancer.target)\n",
    "\n",
    "# Voting Classifier 학습\n",
    "vo_clf.fit(X_train, y_train)\n",
    "\n",
    "# Voting 결과 예측\n",
    "pred = vo_clf.predict(X_test)\n",
    "print('Voting Classifier 정확도 : {0:.4f}'.format(accuracy_score(y_test, pred)))\n",
    "# 다른 분류기를 각각 학습했을 때 결과 예측\n",
    "# classifiers에 lr_clf, knn_clf를 넣어주세요.\n",
    "\n",
    "\n",
    "classifiers = [lr_clf, knn_clf]\n",
    "for classifier in classifiers:\n",
    "    classifier.fit(X_train, y_train)\n",
    "    pred = classifier.predict(X_test)\n",
    "    class_name = classifier.__class__.__name__\n",
    "    print(\"{0} 정확도 : {1:.4f}\".format(class_name, accuracy_score(y_test, pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 앙상블(Ensemble) 학습 (2)\n",
    "### 배깅 (Bagging)\n",
    "배깅(Bagging)은 보팅 (Voting)과 달리 예측할 때 사용하는 분류기를 동일한 분류기로 사용합니다.\n",
    "\n",
    "보팅 (Voting)과의 차이점은 전체 데이터에서 데이터 샘플링을 하여 각 분류기 별로 서로 다른 데이터를 가져가 학습하는 것입니다.\n",
    "\n",
    "배깅의 대표적인 알고리즘으로 랜덤 포레스트가 있습니다. 랜덤 포레스트를 실습해보겠습니다.\n",
    "\n",
    "https://kasausyrzlhe1066469.cdn.ntruss.com/global/file/p/5d1ab3e39640a8964497058d/Bagging.png\n",
    "\n",
    "### 랜덤 포레스트\n",
    "랜덤 포레스트는 앙상블 알고리즘 중 비교적 빠른 속도를 가지고 있고 높은 예측 성능을 보입니다.\n",
    "\n",
    "랜덤 포레스트의 분류기는 의사결정트리를 사용합니다.\n",
    "\n",
    "#### RandomForestClassifier()\n",
    "- n_estimators : 결정 트리 개수 지정\n",
    "- max_depth : 트리의 depth 지정\n",
    "- min_samples_leaf : 리프 노드의 최소 샘플수\n",
    "- min_samples_split : 내부 노드를 분할하는데 필요한 최소 샘플 수\n",
    "\n",
    "### 실습\n",
    "작성된 코드를 보고 따라가며 이해해보세요.\n",
    "\n",
    "param 의 값을 바꿔가며 결과가 어떻게 변하는지 확인해보세요.\n",
    "\n",
    "검증 데이터 예측 정확도가 97%가 넘도록 param을 설정해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최적 파라미터      :  {'max_depth': 2, 'min_samples_leaf': 1, 'min_samples_split': 3, 'n_estimators': 1}\n",
      "최고 예측 정확도   : 0.9121\n",
      "검증 데이터 정확도 : 0.9474\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# 유방암 데이터 세트 불러오기\n",
    "bc = load_breast_cancer()\n",
    "\n",
    "# 학습 데이터와 검증 데이터로 분할하기\n",
    "X_train, X_test, y_train, y_test = train_test_split(bc.data, bc.target, test_size = 0.2, random_state = 121)\n",
    "\n",
    "# RandomForestClassifier 객체 불러오기\n",
    "rfc = RandomForestClassifier()\n",
    "'''\n",
    "RandomForestClassifier()\n",
    "n_estimators : 결정 트리 개수 지정\n",
    "max_depth : 트리의 depth 지정\n",
    "min_samples_leaf : 리프 노드의 최소 샘플수\n",
    "min_samples_split : 내부 노드를 분할하는데 필요한 최소 샘플 수\n",
    "'''\n",
    "# GridSearchCV에 넣을 파라미터\n",
    "param ={\n",
    "    'n_estimators'      : [1],\n",
    "    'max_depth'         : [1,2],\n",
    "    'min_samples_leaf'  : [1,2],\n",
    "    'min_samples_split' : [2,3]\n",
    "}\n",
    "\n",
    "# GridSearchCV 불러오기\n",
    "grid_rfc = GridSearchCV(rfc, param_grid = param)\n",
    "\n",
    "# RandomForestClassifier 학습\n",
    "grid_rfc.fit(X_train, y_train)\n",
    "\n",
    "# 최적의 파라미터와 최고 예측 정확도 계산\n",
    "print('최적 파라미터      : ', grid_rfc.best_params_)\n",
    "print('최고 예측 정확도   : {0:.4f}'.format(grid_rfc.best_score_))\n",
    "\n",
    "# 최적의 파라미터로 학습된 트리 Estimator 가져오기\n",
    "estimator = grid_rfc.best_estimator_\n",
    "\n",
    "# 검증 데이터로 결과 예측\n",
    "pred = estimator.predict(X_test)\n",
    "print('검증 데이터 정확도 : {0:.4f}'.format(accuracy_score(y_test, pred)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 숫자 분류하기\n",
    "사이킷런을 이용해 직접 Digits data를 학습시켜보겠습니다. Digits data는 0 ~ 9까지 숫자들로 이루어진 이미지 데이터입니다.\n",
    "\n",
    "Digits data를 Train 세트와 Test 세트로 나누고 Train 데이터를 K_fold로 나누어 학습을 진행해보세요.\n",
    "\n",
    "이전 실습을 참고해서 DecisionTreeClassifier를 설계하고 정확도를 계산해보세요.\n",
    "\n",
    "## 미션\n",
    "load_digits()로 데이터를 불러오고 feature, label 변수에 data와 target을 따로 저장하세요.\n",
    "\n",
    "train_test_split()로 Train데이터와 Test데이터를 나눠주세요. Train과 Test의 비율은 7:3 으로 정해주세요.\n",
    "\n",
    "param_grid를 설정해주세요.\n",
    "\n",
    "2번에서 만들어진 Train 데이터를 KFold()를 이용해 총 5개의 fold로 나눠주세요.\n",
    "\n",
    "- n_split = 5\n",
    "GridSearchCV로 결정 트리를 학습시키고 최적의 파라미터와 최고 정확도를 뽑아보세요.\n",
    "\n",
    "각 fold별 학습을 진행하고 교차 검증 평균 정확도가 80% 이상 되도록 코딩하세요.\n",
    "\n",
    "- cv_accuracy\n",
    "Test data로 검증한 DecisionTreeClassifier의 정확도가 80% 이상 되도록 코딩하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter : 1, 교차 검증 정확도 : 0.8214\n",
      "Iter : 2, 교차 검증 정확도 : 0.8452\n",
      "Iter : 3, 교차 검증 정확도 : 0.8207\n",
      "Iter : 4, 교차 검증 정확도 : 0.8486\n",
      "Iter : 5, 교차 검증 정확도 : 0.8287\n",
      "교차 검증 평균 정확도 : 0.8329\n",
      "GridSearchCV 최적 파라미터 :  {'max_depth': 10, 'min_samples_split': 3}\n",
      "GridSearchCV 최고 정확도 : 0.7982\n",
      "검증 데이터 정확도 : 0.8426\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# digits 데이터를 불러오고 feature와 label에 data와 target을 저장해주세요.\n",
    "digits = load_digits()\n",
    "feature = digits.data\n",
    "label = digits.target\n",
    "\n",
    "# DecisionTreeClassifier를 불러와주세요.\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "\n",
    "# GridSearchCV에 사용할 Param 값을 정해주세요.\n",
    "param_grid = {'max_depth' : [10,11,12,13], 'min_samples_split' : [3,4,5]}\n",
    "\n",
    "# 파라미터를 설정해주세요.\n",
    "n_splits = 5\n",
    "kfold = KFold(n_splits = n_splits)\n",
    "n_iter = 0\n",
    "cv_accuracy = []\n",
    "\n",
    "# 데이터를 분리해주세요.\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature,label,test_size = 0.3)\n",
    "\n",
    "# X_train 데이터를 K-fold를 사용해서 총 5개의 fold로 나눠주세요.\n",
    "for train_idx, test_idx in kfold.split(X_train):\n",
    "    X_fold_train, X_fold_vali = X_train[train_idx], X_train[test_idx] \n",
    "    y_fold_train, y_fold_vali = y_train[train_idx], y_train[test_idx] \n",
    "    \n",
    "    # GridSearchCV를 이용해 결정 트리를 학습시켜주세요.\n",
    "    grid_dtree = GridSearchCV(dt_clf, param_grid = param_grid, refit= True)\n",
    "    grid_dtree.fit(X_fold_train, y_fold_train)\n",
    "    \n",
    "    # Iter 별 교차 검증 정확도를 계산해보세요.\n",
    "    fold_pred = grid_dtree.predict(X_fold_vali)\n",
    "    \n",
    "    n_iter += 1\n",
    "    print('Iter : {0}, 교차 검증 정확도 : {1:.4f}'.format(n_iter ,accuracy_score(y_fold_vali, fold_pred)))\n",
    "    \n",
    "    # 교차 검증 평균 정확도를 위해 Iter별 교차 검증 정확도를 cv_accuracy에 넣어주세요.\n",
    "    cv_accuracy.append(accuracy_score(y_fold_vali, fold_pred))\n",
    "    \n",
    "# 교차 검증 평균 정확도를 계산해주세요.\n",
    "cv_accuracy = np.array(cv_accuracy)\n",
    "print('교차 검증 평균 정확도 : {0:.4f}'.format(cv_accuracy.mean()))\n",
    "\n",
    "# GricSearchCV 최적 파라미터와 최고 정확도를 출력해보세요.\n",
    "print('GridSearchCV 최적 파라미터 : ', grid_dtree.best_params_)\n",
    "print('GridSearchCV 최고 정확도 : {0:.4f}'.format(grid_dtree.best_score_))\n",
    "    \n",
    "# 최적의 파라미터로 학습된 grid_dtree의 best_estimator를 불러와주세요.\n",
    "estimator = grid_dtree.best_estimator_\n",
    "\n",
    "# 검증 데이터로 결과를 예측해보세요.\n",
    "pred = estimator.predict(X_test)\n",
    "\n",
    "# 검증 데이터 정확도를 출력해보세요.\n",
    "test_accuracy = accuracy_score(pred,y_test)\n",
    "print(\"검증 데이터 정확도 : {0:.4f}\".format(test_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
