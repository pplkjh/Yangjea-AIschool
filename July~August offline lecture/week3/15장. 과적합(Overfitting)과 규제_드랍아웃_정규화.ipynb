{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'elice_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c1aceb017c4b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Keras를 이용한 Augmentation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0melice_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEliceUtils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0meu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEliceUtils\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# example of horizontal shift image augmentation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'elice_utils'"
     ]
    }
   ],
   "source": [
    "#Keras를 이용한 Augmentation\n",
    "# example of horizontal shift image augmentation\n",
    "from numpy import expand_dims\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "# load the image\n",
    "img = load_img('bird.jpg')\n",
    "# convert to numpy array\n",
    "data = img_to_array(img)\n",
    "# expand dimension to one sample\n",
    "samples = expand_dims(data, 0)\n",
    "\n",
    "# create image data augmentation generator\n",
    "# 아래의 6개 옵션 중 하나를 고르거나 여러 옵션을 동시에 주시면 됩니다.\n",
    "datagen = ImageDataGenerator(width_shift_range=[-200,200])\n",
    "# 나머지 5개 옵션\n",
    "# height_shift_range=0.5, horizontal_flip=True, rotation_range=90, brightness_range=[0.2,1.0], zoom_range=[0.5,1.0]\n",
    "\n",
    "# prepare iterator\n",
    "it = datagen.flow(samples, batch_size=1)\n",
    "# generate samples and plot\n",
    "for i in range(9):\n",
    "    # define subplot\n",
    "    plt.subplot(330 + 1 + i)\n",
    "    # generate batch of images\n",
    "    batch = it.next()\n",
    "    # convert to unsigned integers for viewing\n",
    "    image = batch[0].astype('uint8')\n",
    "    # plot raw pixel data\n",
    "    plt.imshow(image)\n",
    "# show the figure\n",
    "# plt.show()\n",
    "# plt.savefig('a.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.1.0) C:\\projects\\opencv-python\\opencv\\modules\\imgproc\\src\\resize.cpp:3718: error: (-215:Assertion failed) !ssize.empty() in function 'cv::resize'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-588ee23f932b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[0mimage_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"cat.jpg\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m \u001b[0mresize_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m450\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m400\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;31m#(y1,y2,x1,x2)(bottom,top,left,right)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-588ee23f932b>\u001b[0m in \u001b[0;36mresize_image\u001b[1;34m(image, w, h)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m#RESIZE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mresize_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mimage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFolder_name\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"/Resize-\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"*\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mExtension\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.1.0) C:\\projects\\opencv-python\\opencv\\modules\\imgproc\\src\\resize.cpp:3718: error: (-215:Assertion failed) !ssize.empty() in function 'cv::resize'\n"
     ]
    }
   ],
   "source": [
    "# OpenCV를 이용한 Augmentation\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "#co-relation between Opencv and Pillow Image Rectangle box\n",
    "# (x1, y1) (left, top)\n",
    "# (right, bottom) (x2, y2)\n",
    "\n",
    "# (top,right,bottom,left)\n",
    "# (32,64,0,0)\n",
    "\n",
    "Folder_name=\"augmented_image\"\n",
    "Extension=\".jpg\"\n",
    "\n",
    "#RESIZE\n",
    "def resize_image(image,w,h):\n",
    "    image=cv2.resize(image,(w,h))\n",
    "    cv2.imwrite(Folder_name+\"/Resize-\"+str(w)+\"*\"+str(h)+Extension, image)\n",
    "\n",
    "#crop\n",
    "def crop_image(image,y1,y2,x1,x2):\n",
    "    image=image[y1:y2,x1:x2]\n",
    "    cv2.imwrite(Folder_name+\"/Crop-\"+str(x1)+str(x2)+\"*\"+str(y1)+str(y2)+Extension, image)\n",
    "\n",
    "def padding_image(image,topBorder,bottomBorder,leftBorder,rightBorder,color_of_border=[0,0,0]):\n",
    "    image = cv2.copyMakeBorder(image,topBorder,bottomBorder,leftBorder,\n",
    "        rightBorder,cv2.BORDER_CONSTANT,value=color_of_border)\n",
    "    cv2.imwrite(Folder_name + \"/padd-\" + str(topBorder) + str(bottomBorder) + \"*\" + str(leftBorder) + str(rightBorder) + Extension, image)\n",
    "\n",
    "def flip_image(image,dir):\n",
    "    image = cv2.flip(image, dir)\n",
    "    cv2.imwrite(Folder_name + \"/flip-\" + str(dir)+Extension, image)\n",
    "\n",
    "def invert_image(image,channel):\n",
    "    # image=cv2.bitwise_not(image)\n",
    "    image=(channel-image)\n",
    "    cv2.imwrite(Folder_name + \"/invert-\"+str(channel)+Extension, image)\n",
    "\n",
    "def add_light(image, gamma=1.0):\n",
    "    invGamma = 1.0 / gamma\n",
    "    table = np.array([((i / 255.0) ** invGamma) * 255\n",
    "                      for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "\n",
    "    image=cv2.LUT(image, table)\n",
    "    if gamma>=1:\n",
    "        cv2.imwrite(Folder_name + \"/light-\"+str(gamma)+Extension, image)\n",
    "    else:\n",
    "        cv2.imwrite(Folder_name + \"/dark-\" + str(gamma) + Extension, image)\n",
    "\n",
    "def add_light_color(image, color, gamma=1.0):\n",
    "    invGamma = 1.0 / gamma\n",
    "    image = (color - image)\n",
    "    table = np.array([((i / 255.0) ** invGamma) * 255\n",
    "                      for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "\n",
    "    image=cv2.LUT(image, table)\n",
    "    if gamma>=1:\n",
    "        cv2.imwrite(Folder_name + \"/light_color-\"+str(gamma)+Extension, image)\n",
    "    else:\n",
    "        cv2.imwrite(Folder_name + \"/dark_color\" + str(gamma) + Extension, image)\n",
    "\n",
    "def saturation_image(image,saturation):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    v = image[:, :, 2]\n",
    "    v = np.where(v <= 255 - saturation, v + saturation, 255)\n",
    "    image[:, :, 2] = v\n",
    "\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_HSV2BGR)\n",
    "    cv2.imwrite(Folder_name + \"/saturation-\" + str(saturation) + Extension, image)\n",
    "\n",
    "def hue_image(image,saturation):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    v = image[:, :, 2]\n",
    "    v = np.where(v <= 255 + saturation, v - saturation, 255)\n",
    "    image[:, :, 2] = v\n",
    "\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_HSV2BGR)\n",
    "    cv2.imwrite(Folder_name + \"/hue-\" + str(saturation) + Extension, image)\n",
    "\n",
    "def scale_image(image,fx,fy):\n",
    "    image = cv2.resize(image,None,fx=fx, fy=fy, interpolation = cv2.INTER_CUBIC)\n",
    "    cv2.imwrite(Folder_name+\"/Scale-\"+str(fx)+str(fy)+Extension, image)\n",
    "\n",
    "def translation_image(image,x,y):\n",
    "    rows, cols ,c= image.shape\n",
    "    M = np.float32([[1, 0, x], [0, 1, y]])\n",
    "    image = cv2.warpAffine(image, M, (cols, rows))\n",
    "    cv2.imwrite(Folder_name + \"/Translation-\" + str(x) + str(y) + Extension, image)\n",
    "\n",
    "def rotate_image(image,deg):\n",
    "    rows, cols,c = image.shape\n",
    "    M = cv2.getRotationMatrix2D((cols/2,rows/2), deg, 1)\n",
    "    image = cv2.warpAffine(image, M, (cols, rows))\n",
    "    cv2.imwrite(Folder_name + \"/Rotate-\" + str(deg) + Extension, image)\n",
    "\n",
    "def transformation_image(image):\n",
    "    rows, cols, ch = image.shape\n",
    "    pts1 = np.float32([[50, 50], [200, 50], [50, 200]])\n",
    "    pts2 = np.float32([[10, 100], [200, 50], [100, 250]])\n",
    "    M = cv2.getAffineTransform(pts1, pts2)\n",
    "    image = cv2.warpAffine(image, M, (cols, rows))\n",
    "    cv2.imwrite(Folder_name + \"/Transformations-\" + str(1) + Extension, image)\n",
    "\n",
    "    pts1 = np.float32([[50, 50], [200, 50], [50, 200]])\n",
    "    pts2 = np.float32([[100, 10], [200, 50], [0, 150]])\n",
    "    M = cv2.getAffineTransform(pts1, pts2)\n",
    "    image = cv2.warpAffine(image, M, (cols, rows))\n",
    "    cv2.imwrite(Folder_name + \"/Transformations-\" + str(2) + Extension, image)\n",
    "\n",
    "    pts1 = np.float32([[50, 50], [200, 50], [50, 200]])\n",
    "    pts2 = np.float32([[100, 10], [200, 50], [30, 175]])\n",
    "    M = cv2.getAffineTransform(pts1, pts2)\n",
    "    image = cv2.warpAffine(image, M, (cols, rows))\n",
    "    cv2.imwrite(Folder_name + \"/Transformations-\" + str(3) + Extension, image)\n",
    "\n",
    "    pts1 = np.float32([[50, 50], [200, 50], [50, 200]])\n",
    "    pts2 = np.float32([[100, 10], [200, 50], [70, 150]])\n",
    "    M = cv2.getAffineTransform(pts1, pts2)\n",
    "    image = cv2.warpAffine(image, M, (cols, rows))\n",
    "    cv2.imwrite(Folder_name + \"/Transformations-\" + str(4) + Extension, image)\n",
    "\n",
    "\n",
    "image_file=\"cat.jpg\"\n",
    "image=cv2.imread(image_file)\n",
    "resize_image(image,450,400)\n",
    "\n",
    "#(y1,y2,x1,x2)(bottom,top,left,right)\n",
    "crop_image(image,100,400,0,350); crop_image(image,100,400,100,450); crop_image(image,0,300,0,350); crop_image(image,0,300,100,450); crop_image(image,100,300,100,350)\n",
    "padding_image(image,100,0,0,0); padding_image(image,0,100,0,0); padding_image(image,0,0,100,0); padding_image(image,0,0,0,100); padding_image(image,100,100,100,100)\n",
    "# 가로, 세로, 둘다\n",
    "flip_image(image,0); flip_image(image,1); flip_image(image,-1)\n",
    "invert_image(image,255); invert_image(image,200); invert_image(image,150); invert_image(image,100); invert_image(image,50)\n",
    "add_light(image,1.5); add_light(image,2.0); add_light(image,2.5); add_light(image,3.0); add_light(image,4.0); add_light(image,5.0); add_light(image,0.7); add_light(image,0.4); add_light(image,0.3); add_light(image,0.1)\n",
    "add_light_color(image,255,1.5); add_light_color(image,200,2.0); add_light_color(image,150,2.5); add_light_color(image,100,3.0); add_light_color(image,50,4.0); add_light_color(image,255,0.7); add_light_color(image,150,0.3); add_light_color(image,100,0.1)\n",
    "saturation_image(image,50); saturation_image(image,100); saturation_image(image,150); saturation_image(image,200)\n",
    "hue_image(image,50); hue_image(image,100); hue_image(image,150); hue_image(image,200)\n",
    "scale_image(image,0.3,0.3); scale_image(image,0.7,0.7); scale_image(image,2,2); scale_image(image,3,3)\n",
    "translation_image(image,150,150); translation_image(image,-150,150); translation_image(image,150,-150); translation_image(image,-150,-150)\n",
    "rotate_image(image,90); rotate_image(image,180); rotate_image(image,270)\n",
    "transformation_image(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 과소적합 (Underfitting)\n",
    "과소적합 (Underfitting)은 학습 데이터를 충분히 학습하지 못하여 테스트 성능도 떨어지는 경우를 말합니다.\n",
    "\n",
    "Underfitting을 야기시키는 원인은 아래와 같은 경우가 있습니다.\n",
    "\n",
    "* 데이터의 개수가 부족한 경우\n",
    "* 데이터의 Bias가 너무 큰 경우\n",
    "* 충분한 학습이 부족한 경우\n",
    "* 데이터에 비해 모델이 너무 단순한 경우\n",
    "\n",
    "IMDb 데이터세트를 이용해 기존 모델과 과소적합 모델을 비교해보겠습니다.\n",
    "\n",
    "이번 실습에서는 데이터 개수와 모델을 조절해보며 과소적합 모델을 학습시켜보고 Train & Test loss가 어떻게 되는지 그래프로 확인해보겠습니다.\n",
    "\n",
    "관련 링크 https://www.tensorflow.org/beta/tutorials/keras/overfit_and_underfit\n",
    "\n",
    "### 실습\n",
    "작성된 코드를 보고 이해해보세요. \n",
    "\n",
    "data_num을 바꿔가며 데이터의 개수에 따라 각 모델의 학습이 어떻게 진행되는지 확인해보세요.\n",
    "\n",
    "keras.layers.Dense()안의 노드 개수를 바꿔가며 모델의 학습이 어떻게 진행되는지 확인해보세요.\n",
    "\n",
    "model.fit() 내부에서 epoch를 바꿔가며 과소적합 되는 모델을 만들어보세요.\n",
    "\n",
    "Visualize() 함수로 Train, Test loss 를 확인하고 과소적합 모델의 결과를 분석해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib.pyplot'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-154e94362fc8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib.pyplot'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "\n",
    "# 각각 리뷰에 따른 데이터 길이가 다르기 때문에 데이터의 Shape을 맞춰줘야합니다.\n",
    "def sequences_shaping(sequences, dimension):\n",
    "    # 0으로 채워진 (len(sequences), dimension) 크기의 행렬을 만듭니다\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, word_indices in enumerate(sequences):\n",
    "        results[i, word_indices] = 1.0  # 각 리뷰 별 빈도수가 높은 단어를 dimension 개수 만큼만 추출하여 사용합니다.\n",
    "        \n",
    "    return results\n",
    "\n",
    "# 시각화 함수\n",
    "def Visualize(histories, key='binary_crossentropy'):\n",
    "    #plt.figure(figsize=(,20))\n",
    "\n",
    "    for name, history in histories:\n",
    "        val = plt.plot(history.epoch, history.history['val_'+key],\n",
    "                   '--', label=name.title()+' Val')\n",
    "        plt.plot(history.epoch, history.history[key], color=val[0].get_color(),\n",
    "             label=name.title()+' Train')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(key.replace('_',' ').title())\n",
    "    plt.legend()\n",
    "\n",
    "    plt.xlim([0,max(history.epoch)])\n",
    "    \n",
    "#     plt.savefig(\"plot.png\")\n",
    "\n",
    "# 100번째 까지 많이 사용하는 단어까지 추출\n",
    "word_num = 100\n",
    "data_num = 60\n",
    "\n",
    "# Keras에 내장되어 있는 imdb 데이터 세트를 불러옵니다.\n",
    "# IMDb 데이터 세트는 Train 25000개 test 25000개로 이루어져 있습니다.\n",
    "(train_data, train_labels), (test_data, test_labels) = keras.datasets.imdb.load_data(num_words=word_num)\n",
    "\n",
    "# 데이터 Shape을 맞춰주기 위한 sequence 함수를 불러옵니다.\n",
    "train_data = sequences_shaping(train_data, dimension=word_num)\n",
    "test_data = sequences_shaping(test_data, dimension=word_num)\n",
    "\n",
    "# 메모리 효율 및 과소적합을 위해 데이터 중 data_num개만 사용합니다.\n",
    "train_data = train_data[:data_num,:]\n",
    "test_data = test_data[:data_num,:]\n",
    "train_labels = train_labels[:data_num]\n",
    "test_labels =test_labels[:data_num]\n",
    "\n",
    "# 과소적합 경우와 비교하기 위해 기본 모델을 하나 만들어줍니다.\n",
    "\n",
    "basic_model = keras.Sequential([\n",
    "    # `.summary` 메서드 때문에 `input_shape`가 필요합니다\n",
    "    # 첫 번째 Layer에 데이터를 넣을때는 input_shape을 맞춰줘야합니다.\n",
    "    keras.layers.Dense(16, activation=tf.nn.relu, input_shape=(word_num,)),\n",
    "    keras.layers.Dense(16, activation=tf.nn.relu),\n",
    "    # Regression 이므로 마지막 Node는 1개로 고정해주세요.\n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "])\n",
    "\n",
    "# 과소적합 모델입니다.\n",
    "underfitting_model = keras.Sequential([\n",
    "    keras.layers.Dense(2, activation=tf.nn.relu, input_shape=(word_num,)),\n",
    "    keras.layers.Dense(2, activation=tf.nn.relu),\n",
    "    # Regression 이므로 마지막 Node는 1개로 고정해주세요.\n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "])\n",
    "\n",
    "# 기존 모델을 학습시킬 최적화 방법, loss 계산 방법, 평가 방법을 설정합니다.\n",
    "basic_model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy', 'binary_crossentropy'])\n",
    "# 현재 모델이 어떻게 이루어져있는지 출력합니다.\n",
    "basic_model.summary()\n",
    "# 모델을 학습시킵니다.\n",
    "baseline_history = basic_model.fit(train_data,train_labels,epochs=20,batch_size=500,validation_data=(test_data, test_labels), verbose=2)\n",
    "\n",
    "# 기존 모델을 학습시킬 최적화 방법, loss 계산 방법, 평가 방법을 설정합니다.\n",
    "underfitting_model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy', 'binary_crossentropy'])\n",
    "# 현재 모델이 어떻게 이루어져있는지 출력합니다.\n",
    "underfitting_model.summary()\n",
    "# 모델을 학습시킵니다.\n",
    "# Epoch를 바꿔가며 모델을 Underfitting 시켜보세요.\n",
    "underfitting_history = underfitting_model.fit(train_data, train_labels, epochs=10, batch_size=500, validation_data=(test_data, test_labels),verbose=2)\n",
    "\n",
    "\n",
    "# 각 모델 별 Loss 그래프를 그려줍니다.\n",
    "Visualize([('basic', baseline_history),('Underfitting', underfitting_history)])\n",
    "              \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 과대적합 (Overfitting)\n",
    "과대적합 (Overfitting)은 모델이 학습 데이터에만 너무 치중되어 학습 데이터의 예측 성능은 좋으나 테스트 데이터의 성능이 떨어지는 경우를 말합니다.\n",
    "\n",
    "모델이 과대적합이 되면 일반화 되지 않은 모델이라고도 합니다.\n",
    "\n",
    "과대적합을 야기시키는 원인은 아래와 같습니다.\n",
    "\n",
    "* 데이터의 Variance가 높은 경우\n",
    "* 너무 많은 Epoch로 학습 데이터를 학습시킨 경우\n",
    "* 학습에 사용된 파라미터가 너무 많은 경우\n",
    "* 데이터에 비해 모델이 너무 복잡한 경우\n",
    "* 데이터에 노이즈 & Outlier가 너무 많은 경우\n",
    "\n",
    "이번 실습에서는 일부러 과대적합된 모델을 만들어 보고 Loss 그래프를 통해 기존 모델과 어떻게 성능 차이가 나는지 확인해보겠습니다.\n",
    "\n",
    "관련 링크 https://www.tensorflow.org/beta/tutorials/keras/overfit_and_underfit\n",
    "### 실습\n",
    "작성된 코드를 보고 이해해보세요.\n",
    "\n",
    "data_num을 바꿔가며 데이터의 개수에 따라 각 모델의 학습이 어떻게 진행되는지 확인해보세요.\n",
    "\n",
    "keras.layers.Dense()안의 뉴런 개수를 바꿔가며 모델의 학습이 어떻게 진행되는지 확인해보세요.\n",
    "\n",
    "model.fit() 내부에서 epoch를 바꿔가며 과소적합 되는 모델을 만들어보세요.\n",
    "\n",
    "Visualize() 함수로 Train, Test loss 를 확인하고 과소적합 모델의 결과를 분석해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "\n",
    "# 각각 리뷰에 따른 데이터 길이가 다르기 때문에 데이터의 Shape을 맞춰줘야합니다.\n",
    "def sequences_shaping(sequences, dimension):\n",
    "    # 0으로 채워진 (len(sequences), dimension) 크기의 행렬을 만듭니다\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, word_indices in enumerate(sequences):\n",
    "        results[i, word_indices] = 1.0  # 각 리뷰 별 빈도수가 높은 단어를 dimension 개수 만큼만 추출하여 사용합니다.\n",
    "        \n",
    "    return results\n",
    "\n",
    "# 시각화 함수\n",
    "def Visulaize(histories, key='binary_crossentropy'):\n",
    "    #plt.figure(figsize=(,20))\n",
    "\n",
    "    for name, history in histories:\n",
    "        val = plt.plot(history.epoch, history.history['val_'+key],\n",
    "                   '--', label=name.title()+' Val')\n",
    "        plt.plot(history.epoch, history.history[key], color=val[0].get_color(),\n",
    "             label=name.title()+' Train')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(key.replace('_',' ').title())\n",
    "    plt.legend()\n",
    "\n",
    "    plt.xlim([0,max(history.epoch)])\n",
    "    \n",
    "#     plt.savefig(\"plot.png\")\n",
    "\n",
    "# 100번째 까지 많이 사용하는 단어까지 추출\n",
    "word_num = 100\n",
    "data_num = 250000\n",
    "\n",
    "# Keras에 내장되어 있는 imdb 데이터 세트를 불러옵니다.\n",
    "# IMDb 데이터 세트는 Train 25000개 test 25000개로 이루어져 있습니다.\n",
    "(train_data, train_labels), (test_data, test_labels) = keras.datasets.imdb.load_data(num_words=word_num)\n",
    "\n",
    "# 데이터 Shape을 맞춰주기 위한 sequence 함수를 불러옵니다.\n",
    "train_data = sequences_shaping(train_data, dimension=word_num)\n",
    "test_data = sequences_shaping(test_data, dimension=word_num)\n",
    "\n",
    "# 메모리 효율 및 과대적합을 위해 데이터 중 data_num개만 사용합니다.\n",
    "train_data = train_data[:data_num,:]\n",
    "test_data = test_data[:data_num,:]\n",
    "train_labels = train_labels[:data_num]\n",
    "test_labels =test_labels[:data_num]\n",
    "\n",
    "# 과대적합 경우와 비교하기 위해 기본 모델을 하나 만들어줍니다.\n",
    "basic_model = keras.Sequential([\n",
    "    # `.summary` 메서드 때문에 `input_shape`가 필요합니다\n",
    "    # 첫 번째 Layer에 데이터를 넣을때는 input_shape을 맞춰줘야합니다.\n",
    "    keras.layers.Dense(16, activation=tf.nn.relu, input_shape=(word_num,)),\n",
    "    keras.layers.Dense(16, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "])\n",
    "\n",
    "# Neuron 개수를 바꿔가면서 과대적합 모델을 만들어보세요.\n",
    "overfitting_model = keras.Sequential([\n",
    "    keras.layers.Dense(5, activation=tf.nn.relu, input_shape=(word_num,)),\n",
    "    keras.layers.Dense(5, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "])\n",
    "\n",
    "# 기존 모델을 학습시킬 최적화 방법, loss 계산 방법, 평가 방법을 설정합니다.\n",
    "basic_model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy', 'binary_crossentropy'])\n",
    "# 현재 모델이 어떻게 이루어져있는지 출력합니다.\n",
    "basic_model.summary()\n",
    "# 모델을 학습시킵니다.\n",
    "basic_history = basic_model.fit(train_data,train_labels,epochs=20,batch_size=500,validation_data=(test_data, test_labels), verbose=2)\n",
    "\n",
    "# 기존 모델을 학습시킬 최적화 방법, loss 계산 방법, 평가 방법을 설정합니다.\n",
    "overfitting_model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy', 'binary_crossentropy'])\n",
    "# 현재 모델이 어떻게 이루어져있는지 출력합니다.\n",
    "overfitting_model.summary()\n",
    "# 모델을 학습시킵니다.\n",
    "# Epoch를 바꿔가면서 학습시켜보세요.\n",
    "overfitting_history = overfitting_model.fit(train_data, train_labels, epochs=10, batch_size=500, validation_data=(test_data, test_labels),verbose=2)\n",
    "\n",
    "\n",
    "# 각 모델 별 Loss 그래프를 그려줍니다.\n",
    "Visulaize([('Basic', basic_history),('Onderfitting', overfitting_history)])\n",
    "              \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 드롭 아웃 (Drop out)\n",
    "드롭 아웃 (Drop Out)은 모델이 과적합되는 것을 막기 위한 가장 보편적인 Regularization 기법 중 하나입니다.\n",
    "\n",
    "https://kasausyrzlhe1066469.cdn.ntruss.com/global/file/p/5d27dc6afa5d6f586dc75417/Dropout.png\n",
    "\n",
    "드롭 아웃은 데이터를 학습할 때, 일부 뉴런을 랜덤하게 0으로 만들어 모델 내부의 특정 Weight에 치중되는 것을 막습니다.\n",
    "\n",
    "이를 통해 모델이 특정 데이터에 치중되는 것을 막고 일반화된 모델을 만들 수 있습니다.\n",
    "\n",
    "드롭 아웃을 사용하는데 있어 주의할 점은 학습이 끝난 후 테스트 과정에서는 드롭 아웃을 사용하면 안된다는 점입니다.\n",
    "\n",
    "이번 실습에서 드롭 아웃을 적용한 모델과 적용하지 않은 모델의 Loss 차이를 보겠습니다.\n",
    "\n",
    "관련 링크 https://www.tensorflow.org/beta/tutorials/keras/overfit_and_underfit\n",
    "### 실습\n",
    "#### Drop-out Layer\n",
    "\n",
    "* keras.layers.Dropout(prob)\n",
    "    * prob : 드롭 아웃을 적용할 확률 (0.1 ~ 0.5)\n",
    "    \n",
    "작성된 코드를 보며 이해해보세요.\n",
    "\n",
    "dropout_model에 Dropout Layer를 추가해보세요.\n",
    "\n",
    "basic_model과dropout_model의 Loss 그래프를 확인해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib.pyplot'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-2ffe008eda29>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib.pyplot'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "\n",
    "# 각각 리뷰에 따른 데이터 길이가 다르기 때문에 데이터의 Shape을 맞춰줘야합니다.\n",
    "def sequences_shaping(sequences, dimension):\n",
    "    # 0으로 채워진 (len(sequences), dimension) 크기의 행렬을 만듭니다\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, word_indices in enumerate(sequences):\n",
    "        results[i, word_indices] = 1.0  # 각 리뷰 별 빈도수가 높은 단어를 dimension 개수 만큼만 추출하여 사용합니다.\n",
    "        \n",
    "    return results\n",
    "\n",
    "# 시각화 함수\n",
    "def Visulaize(histories, key='binary_crossentropy'):\n",
    "    #plt.figure(figsize=(,20))\n",
    "\n",
    "    for name, history in histories:\n",
    "        val = plt.plot(history.epoch, history.history['val_'+key],\n",
    "                   '--', label=name.title()+' Val')\n",
    "        plt.plot(history.epoch, history.history[key], color=val[0].get_color(),\n",
    "             label=name.title()+' Train')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(key.replace('_',' ').title())\n",
    "    plt.legend()\n",
    "\n",
    "    plt.xlim([0,max(history.epoch)])\n",
    "    \n",
    "#     plt.savefig(\"plot.png\")\n",
    "\n",
    "# 100번째 까지 많이 사용하는 단어까지 추출\n",
    "word_num = 100\n",
    "data_num = 20000\n",
    "\n",
    "# Keras에 내장되어 있는 imdb 데이터 세트를 불러옵니다.\n",
    "# IMDb 데이터 세트는 Train 25000개 test 25000개로 이루어져 있습니다.\n",
    "(train_data, train_labels), (test_data, test_labels) = keras.datasets.imdb.load_data(num_words=word_num)\n",
    "\n",
    "# 데이터 Shape을 맞춰주기 위한 sequence 함수를 불러옵니다.\n",
    "train_data = sequences_shaping(train_data, dimension=word_num)\n",
    "test_data = sequences_shaping(test_data, dimension=word_num)\n",
    "\n",
    "# 메모리 효율을 위해 데이터 중 data_num개만 사용합니다.\n",
    "train_data = train_data[:data_num,:]\n",
    "test_data = test_data[:data_num,:]\n",
    "train_labels = train_labels[:data_num]\n",
    "test_labels =test_labels[:data_num]\n",
    "\n",
    "# 드롭 아웃을 적용한 모델과 비교하기 위해 기본 모델을 하나 만들어줍니다.\n",
    "\n",
    "basic_model = keras.Sequential([\n",
    "    # `.summary` 메서드 때문에 `input_shape`가 필요합니다\n",
    "    # 첫 번째 Layer에 데이터를 넣을때는 input_shape을 맞춰줘야합니다.\n",
    "    keras.layers.Dense(64, activation=tf.nn.relu, input_shape=(word_num,)),\n",
    "    keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "])\n",
    "\n",
    "# 각 Layer에 Dropout이 적용된 모델을 만들어보세요.\n",
    "dropout_model = keras.Sequential([\n",
    "    keras.layers.Dense(64, activation=tf.nn.relu, input_shape=(word_num,)),\n",
    "    # Dense (Fully Connected) Layer에 드롭 아웃을 적용해보세요.\n",
    "    keras.layers.Dropout(0.3),\n",
    "    keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "    # Dense (Fully Connected) Layer에 드롭 아웃을 적용해보세요.\n",
    "    keras.layers.Dropout(0.4),\n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "])\n",
    "\n",
    "# 기존 모델을 학습시킬 최적화 방법, loss 계산 방법, 평가 방법을 설정합니다.\n",
    "basic_model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy', 'binary_crossentropy'])\n",
    "# 현재 모델이 어떻게 이루어져있는지 출력합니다.\n",
    "basic_model.summary()\n",
    "# 모델을 학습시킵니다.\n",
    "basic_history = basic_model.fit(train_data,train_labels,epochs=20,batch_size=500,validation_data=(test_data, test_labels), verbose=2)\n",
    "\n",
    "# 기존 모델을 학습시킬 최적화 방법, loss 계산 방법, 평가 방법을 설정합니다.\n",
    "dropout_model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy', 'binary_crossentropy'])\n",
    "# 현재 모델이 어떻게 이루어져있는지 출력합니다.\n",
    "dropout_model.summary()\n",
    "# Drop out이 적용된 모델을 학습시킵니다.\n",
    "dropout_history = dropout_model.fit(train_data, train_labels, epochs=20, batch_size=500, validation_data=(test_data, test_labels),verbose=2)\n",
    "\n",
    "\n",
    "# 각 모델 별 Loss 그래프를 그려줍니다.\n",
    "Visulaize([('Basic', basic_history),('Dropout', dropout_history)])\n",
    "              \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1 Regularization\n",
    "L1 Regularization은 가중치 (Weight)의 절댓값에 비례하는 Loss가 기존 손실 함수에 추가되는 형태입니다.\n",
    "\n",
    "Total Loss = Loss + \\lambda\\sum_w|W| \n",
    "\n",
    "L1 Regularization은 모델 내의 일부 가중치를 0으로 만들어 의미있는 가중치만 남도록 만들어줍니다. 이를 통해 모델을 일반화시킬 수 있습니다. 다른 말로 Sparse Model을 만든다 라고도 합니다.\n",
    "\n",
    "L1 Regularization을 모델에 적용하기 위해 kernel.layers.Dense() 내부에 인자로 keras.regularizers.l1(ratio)을 추가할 수 있습니다.\n",
    "\n",
    "L1 Regularization -keras.regularizers.l1(ratio) - ratio : L1을 적용하는 비율 (0.001 ~0.005)\n",
    "\n",
    "* keras.layers.Dense(kernel_regularizer, bias_regularizer, activity_regularizer)\n",
    "    * kernel_regularizer : Weight에 Regularization 적용\n",
    "    * bias_regularizer : Bias에 Regularization 적용\n",
    "    * activity_regularizer : Output y에 Regularization 적용\n",
    "    \n",
    "관련 링크 https://www.tensorflow.org/beta/tutorials/keras/overfit_and_underfit https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Dense\n",
    "### 실습\n",
    "작성된 코드를 보며 이해해보세요.\n",
    "\n",
    "L1_regularization_model()에 L1 regularization을 추가해보세요.\n",
    "\n",
    "basic_model과L1_regularization_model의 Loss 그래프를 확인하고 과적합을 얼마나 완화시켰는지 분석해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib.pyplot'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-eb9f4b2a4d71>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib.pyplot'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "# 각각 리뷰에 따른 데이터 길이가 다르기 때문에 데이터의 Shape을 맞춰줘야합니다.\n",
    "def sequences_shaping(sequences, dimension):\n",
    "    # 0으로 채워진 (len(sequences), dimension) 크기의 행렬을 만듭니다\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, word_indices in enumerate(sequences):\n",
    "        results[i, word_indices] = 1.0  # 각 리뷰 별 빈도수가 높은 단어를 dimension 개수 만큼만 추출하여 사용합니다.\n",
    "        \n",
    "    return results\n",
    "\n",
    "# 시각화 함수\n",
    "def Visulaize(histories, key='binary_crossentropy'):\n",
    "    #plt.figure(figsize=(,20))\n",
    "\n",
    "    for name, history in histories:\n",
    "        val = plt.plot(history.epoch, history.history['val_'+key],\n",
    "                   '--', label=name.title()+' Val')\n",
    "        plt.plot(history.epoch, history.history[key], color=val[0].get_color(),\n",
    "             label=name.title()+' Train')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(key.replace('_',' ').title())\n",
    "    plt.legend()\n",
    "\n",
    "    plt.xlim([0,max(history.epoch)])\n",
    "    \n",
    "#     plt.savefig(\"plot.png\")\n",
    "\n",
    "# 100번째 까지 많이 사용하는 단어까지 추출\n",
    "word_num = 100\n",
    "data_num = 25000\n",
    "\n",
    "# Keras에 내장되어 있는 imdb 데이터 세트를 불러옵니다.\n",
    "# IMDb 데이터 세트는 Train 25000개 test 25000개로 이루어져 있습니다.\n",
    "(train_data, train_labels), (test_data, test_labels) = keras.datasets.imdb.load_data(num_words=word_num)\n",
    "\n",
    "# 데이터 Shape을 맞춰주기 위한 sequence 함수를 불러옵니다.\n",
    "train_data = sequences_shaping(train_data, dimension=word_num)\n",
    "test_data = sequences_shaping(test_data, dimension=word_num)\n",
    "\n",
    "# L1 Regularization 모델와 비교하기 위해 기본 모델을 하나 만들어줍니다.\n",
    "basic_model = keras.Sequential([\n",
    "    # `.summary` 메서드 때문에 `input_shape`가 필요합니다\n",
    "    # 첫 번째 Layer에 데이터를 넣을때는 input_shape을 맞춰줘야합니다.\n",
    "    keras.layers.Dense(128, activation=tf.nn.relu, input_shape=(word_num,)),\n",
    "    keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "])\n",
    "\n",
    "# 기존 모델에 L1 Regularization을 적용해보겠습니다.\n",
    "L1_regularization_model = keras.Sequential([\n",
    "    # 각 Dense (Fully Connected) Layer에 인자로 keras.regularizers.l1을 적용해보세요.\n",
    "    # Kernel, bias, activity에 하나씩 적용해보며 결과를 비교해보세요.\n",
    "    keras.layers.Dense(128, activation=tf.nn.relu, input_shape=(word_num,), kernel_regularizer= keras.regularizers.l1(0.001)),\n",
    "    keras.layers.Dense(128, activation=tf.nn.relu, kernel_regularizer = keras.regularizers.l1(0.005)),\n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "])\n",
    "\n",
    "# 기존 모델을 학습시킬 최적화 방법, loss 계산 방법, 평가 방법을 설정합니다.\n",
    "basic_model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy', 'binary_crossentropy'])\n",
    "# 현재 모델이 어떻게 이루어져있는지 출력합니다.\n",
    "basic_model.summary()\n",
    "# 모델을 학습시킵니다.\n",
    "basic_history = basic_model.fit(train_data,train_labels,epochs=20,batch_size=500,validation_data=(test_data, test_labels), verbose=2)\n",
    "\n",
    "# L1 모델을 학습시킬 최적화 방법, loss 계산 방법, 평가 방법을 설정합니다.\n",
    "L1_regularization_model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy', 'binary_crossentropy'])\n",
    "# L1 모델이 어떻게 이루어져있는지 출력합니다.\n",
    "L1_regularization_model.summary()\n",
    "# L1 모델을 학습시킵니다.\n",
    "L1_regularization_history = L1_regularization_model.fit(train_data, train_labels, epochs=20, batch_size=500, validation_data=(test_data, test_labels),verbose=2)\n",
    "\n",
    "\n",
    "# 각 모델 별 Loss 그래프를 그려줍니다.\n",
    "Visulaize([('Basic', basic_history),('L1 Regularization', L1_regularization_history)])\n",
    "              \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2 Regularization\n",
    "L2 Regularization은 가중치 (Weight)의 제곱에 비례하는 Loss가 기존 손실 함수에 추가되는 형태입니다.\n",
    "\n",
    "Total Loss = Loss + \\lambda\\sum_wW^2  \n",
    "\n",
    "L2 Regularization은 학습이 진행될 때 가중치의 값이 0에 가까워지도록 만들어줍니다. 가중치를 0으로 만들어주는 L1 Regularization와는 차이가 있습니다.\n",
    "\n",
    "이를 통해 특정 가중치에 치중되지 않도록 가중치 값을 조율하게 되며 Weight Decay 라고도 부릅니다.\n",
    "\n",
    "L1 Regularization과 같이 kernel.layers.Dense() 내부에 인자로 keras.regularizers.l2(ratio)을 추가할 수 있습니다.\n",
    "\n",
    "L2 Regularization -keras.regularizers.l2(ratio) - ratio : L2을 적용하는 비율 (0.001 ~0.005)\n",
    "\n",
    "* keras.layers.Dense(kernel_regularizer, bias_regularizer, activity_regularizer)\n",
    "    * kernel_regularizer : Weight에 Regularization 적용\n",
    "    * bias_regularizer : Bias에 Regularization 적용\n",
    "    * activity_regularizer : Output y에 Regularization 적용\n",
    "    \n",
    "관련 링크 https://www.tensorflow.org/beta/tutorials/keras/overfit_and_underfit https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Dense\n",
    "### 실습\n",
    "작성된 코드를 보며 이해해보세요.\n",
    "\n",
    "L2_regularization_model()에 L2regularization을 추가해보세요.\n",
    "\n",
    "basic_model과L2_regularization_model의 Loss 그래프를 확인하고 과적합을 얼마나 완화시켰는지 분석해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'rcParams'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-aba6156dbef3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcycler\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcycler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolorbar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrcsetup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstyle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\colorbar.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmpl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0martist\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmartist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcbook\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollections\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\artist.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrcParams\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m from .transforms import (Bbox, IdentityTransform, Transform, TransformedBbox,\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'rcParams'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "\n",
    "# 각각 리뷰에 따른 데이터 길이가 다르기 때문에 데이터의 Shape을 맞춰줘야합니다.\n",
    "def sequences_shaping(sequences, dimension):\n",
    "    # 0으로 채워진 (len(sequences), dimension) 크기의 행렬을 만듭니다\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, word_indices in enumerate(sequences):\n",
    "        results[i, word_indices] = 1.0  # 각 리뷰 별 빈도수가 높은 단어를 dimension 개수 만큼만 추출하여 사용합니다.\n",
    "        \n",
    "    return results\n",
    "\n",
    "# 시각화 함수\n",
    "def Visulaize(histories, key='binary_crossentropy'):\n",
    "    #plt.figure(figsize=(,20))\n",
    "\n",
    "    for name, history in histories:\n",
    "        val = plt.plot(history.epoch, history.history['val_'+key],\n",
    "                   '--', label=name.title()+' Val')\n",
    "        plt.plot(history.epoch, history.history[key], color=val[0].get_color(),\n",
    "             label=name.title()+' Train')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(key.replace('_',' ').title())\n",
    "    plt.legend()\n",
    "\n",
    "    plt.xlim([0,max(history.epoch)])\n",
    "    \n",
    "#     plt.savefig(\"plot.png\")\n",
    "\n",
    "# 100번째 까지 많이 사용하는 단어까지 추출\n",
    "word_num = 100\n",
    "data_num = 25000\n",
    "\n",
    "# Keras에 내장되어 있는 imdb 데이터 세트를 불러옵니다.\n",
    "# IMDb 데이터 세트는 Train 25000개 test 25000개로 이루어져 있습니다.\n",
    "(train_data, train_labels), (test_data, test_labels) = keras.datasets.imdb.load_data(num_words=word_num)\n",
    "\n",
    "# 데이터 Shape을 맞춰주기 위한 sequence 함수를 불러옵니다.\n",
    "train_data = sequences_shaping(train_data, dimension=word_num)\n",
    "test_data = sequences_shaping(test_data, dimension=word_num)\n",
    "\n",
    "# L2 Regularization 모델와 비교하기 위해 기본 모델을 하나 만들어줍니다.\n",
    "basic_model = keras.Sequential([\n",
    "    # `.summary` 메서드 때문에 `input_shape`가 필요합니다\n",
    "    # 첫 번째 Layer에 데이터를 넣을때는 input_shape을 맞춰줘야합니다.\n",
    "    keras.layers.Dense(128, activation=tf.nn.relu, input_shape=(word_num,)),\n",
    "    keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "])\n",
    "\n",
    "# 기존 모델에 L2 Regularization을 적용해보겠습니다.\n",
    "L2_regularization_model = keras.Sequential([\n",
    "    # 각 Dense (Fully Connected) Layer에 인자로 keras.regularizers.l1을 적용해보세요.\n",
    "    # Kernel, bias, activity에 하나씩 적용해보며 결과를 비교해보세요.\n",
    "    keras.layers.Dense(128, activation=tf.nn.relu, input_shape=(word_num,), kernel_regularizer= keras.regularizers.l2(0.005)),\n",
    "    keras.layers.Dense(128, activation=tf.nn.relu, kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "])\n",
    "\n",
    "# 기존 모델을 학습시킬 최적화 방법, loss 계산 방법, 평가 방법을 설정합니다.\n",
    "basic_model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy', 'binary_crossentropy'])\n",
    "# 현재 모델이 어떻게 이루어져있는지 출력합니다.\n",
    "basic_model.summary()\n",
    "# 모델을 학습시킵니다.\n",
    "basic_history = basic_model.fit(train_data,train_labels,epochs=20,batch_size=500,validation_data=(test_data, test_labels), verbose=2)\n",
    "\n",
    "# L2 모델을 학습시킬 최적화 방법, loss 계산 방법, 평가 방법을 설정합니다.\n",
    "L2_regularization_model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy', 'binary_crossentropy'])\n",
    "# L2 모델이 어떻게 이루어져있는지 출력합니다.\n",
    "L2_regularization_model.summary()\n",
    "# L2 모델을 학습시킵니다.\n",
    "L2_regularization_history = L2_regularization_model.fit(train_data, train_labels, epochs=20, batch_size=500, validation_data=(test_data, test_labels),verbose=2)\n",
    "\n",
    "\n",
    "# 각 모델 별 Loss 그래프를 그려줍니다.\n",
    "Visulaize([('Basic', basic_history),('L2 Regularization', L2_regularization_history)])\n",
    "              \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Well-Trained 모델 만들기\n",
    "IMDb 데이터로 DNN을 학습시켜 보겠습니다.\n",
    "\n",
    "Keras를 이용해 직접 만든 모델을 학습시키고 과소적함, 과대적합 되지 않도록 모델을 finetuning 해보세요.\n",
    "\n",
    "Visualize()를 통해 Loss를 확인해보고 모델이 어떻게 학습되었는지 분석해보세요.\n",
    "\n",
    "최종적으로 검증 정확도가 85% 이상이 되도록 네트워크를 구현해보세요.\n",
    "\n",
    "### 미션\n",
    "불러온 IMDb 데이터의 Shape을 확인해보세요.\n",
    "\n",
    "L1_ratio, L2_ratio 를 설정하고 keras.regularizers를 선언해주세요.\n",
    "\n",
    "keras.Sequential()로 model을 만들어보세요.\n",
    "\n",
    "keras.layers.Dense() 를 쌓아 Layer를 추가해보세요.\n",
    "\n",
    "Dropout, L1, L2 regularization 들을 적용해 model이 잘 학습되도록 만들어보세요.\n",
    "\n",
    "test_accuracy가 85% 이상되도록 모델을 학습시켜보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'rcParams'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-6dbf2d3b5579>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcycler\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcycler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolorbar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrcsetup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstyle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\colorbar.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmpl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0martist\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmartist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcbook\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollections\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\artist.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrcParams\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m from .transforms import (Bbox, IdentityTransform, Transform, TransformedBbox,\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'rcParams'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "\n",
    "# 시각화 함수\n",
    "def Visulaize(histories, key='binary_crossentropy'):\n",
    "    for name, history in histories:\n",
    "        val = plt.plot(history.epoch, history.history['val_'+key],\n",
    "                   '--', label=name.title()+' Val')\n",
    "        plt.plot(history.epoch, history.history[key], color=val[0].get_color(),\n",
    "             label=name.title()+' Train')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(key.replace('_',' ').title())\n",
    "    plt.legend()\n",
    "\n",
    "    plt.xlim([0,max(history.epoch)])\n",
    "    \n",
    "#     plt.savefig(\"plot.png\")\n",
    "\n",
    "\n",
    "word_num = 1000\n",
    "data_num = 25000\n",
    "\n",
    "# data 폴더에 있는 IMDb 데이터들을 불러옵니다.\n",
    "# 각 데이터의 Shape을 확인하고 모델에 넣어주세요. np.shape() 사용\n",
    "train_data = np.loadtxt('./data/train_data.txt', delimiter =',', dtype = np.float32)\n",
    "train_labels = np.loadtxt('./data/train_labels.txt', delimiter =',', dtype = np.float32)\n",
    "test_data = np.loadtxt('./data/test_data.txt', delimiter =',', dtype = np.float32)\n",
    "test_labels = np.loadtxt('./data/test_labels.txt', delimiter =',', dtype = np.float32)\n",
    "\n",
    "# L1, L2 ratio를 설정해주세요.\n",
    "L1_ratio = 0.002\n",
    "L2_ratio = 0.004\n",
    "\n",
    "# Keras에 내장되어 있는 regularizers를 선언해주세요.\n",
    "L1_regularizer = keras.regularizers.l1(L1_ratio)\n",
    "L2_regularizer = keras.regularizers.l2(L2_ratio)\n",
    "\n",
    "\n",
    "# IMDb 데이터를 학습할 모델을 만들어주세요.\n",
    "model = keras.Sequential([\n",
    "    # `.summary` 메서드 때문에 `input_shape`가 필요합니다\n",
    "    # 첫 번째 Layer에 데이터를 넣을때는 input_shape을 맞춰줘야합니다.\n",
    "    # Layer를 선언해 모델을 만들어보세요.\n",
    "    keras.layers.Dense(128, activation=tf.nn.relu, input_shape=(word_num,), kernel_regularizer= L1_regularizer),\n",
    "    keras.layers.Dropout(0.0002),\n",
    "    keras.layers.Dense(128, activation=tf.nn.relu, kernel_regularizer= L2_regularizer),\n",
    "    keras.layers.Dropout(0.0002),\n",
    "    # 마지막 Layer\n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "])\n",
    "\n",
    "# 기존 모델을 학습시킬 최적화 방법, loss 계산 방법, 평가 방법을 설정합니다.\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy', 'binary_crossentropy'])\n",
    "# 현재 모델이 어떻게 이루어져있는지 출력합니다.\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# 모델을 학습시킵니다.\n",
    "history = model.fit(train_data,train_labels,epochs=20,batch_size=500,validation_data=(test_data, test_labels), verbose=2)\n",
    "\n",
    "# 학습된 모델을 Test_data로 검증해주세요.\n",
    "pred = model.predict(test_data)\n",
    "loss, test_accuracy, _ = model.evaluate(test_data, test_labels)\n",
    "print('검증 정확도 :', test_accuracy)\n",
    "\n",
    "# 모델의 Loss 그래프를 그려줍니다.\n",
    "Visulaize([('Basic', history)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
