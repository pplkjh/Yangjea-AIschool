{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting train-images-idx3-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting train-labels-idx1-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting t10k-images-idx3-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting t10k-labels-idx1-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Creating pickle file ...\n",
      "Done!\n",
      "train_x's shape: (11272, 784)\n",
      "test_x's shape: (1866, 784)\n",
      "loop: 0 0.7043777293828246\n",
      "loop: 100 0.3094035952916154\n",
      "loop: 200 0.19106252178519784\n",
      "loop: 300 0.15772416545553872\n",
      "loop: 400 0.14255528361091024\n",
      "loop: 500 0.1336554274346174\n",
      "loop: 600 0.12762011895982686\n",
      "loop: 700 0.12313725586696012\n",
      "loop: 800 0.11959735791192314\n",
      "loop: 900 0.11667822443364367\n",
      "loop: 999 0.11421530024777592\n",
      "Accuracy: 0.9599893541518808\n",
      "Accuracy: 0.9598070739549842\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 역전파법\n",
    "import matplotlib.pylab as plt\n",
    "import scratch.dataset.mnist as mnist\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ANN:\n",
    "    def __init__(self, layers_size):\n",
    "        self.layers_size = layers_size\n",
    "        self.parameters = {}\n",
    "        self.L = len(self.layers_size)\n",
    "        self.n = 0\n",
    "        self.costs = []\n",
    "\n",
    "    def sigmoid(self, Z):\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        np.random.seed(1)\n",
    "\n",
    "        for l in range(1, len(self.layers_size)):\n",
    "            self.parameters[\"W\" + str(l)] = np.random.randn(self.layers_size[l], self.layers_size[l - 1]) / np.sqrt(\n",
    "                self.layers_size[l - 1])\n",
    "            self.parameters[\"b\" + str(l)] = np.zeros((self.layers_size[l], 1))\n",
    "\n",
    "    def forward(self, X):\n",
    "        store = {}\n",
    "        A = X.T\n",
    "\n",
    "        for l in range(self.L - 1):\n",
    "            Z = self.parameters[\"W\" + str(l + 1)].dot(A) + self.parameters[\"b\" + str(l + 1)]\n",
    "            A = self.sigmoid(Z)\n",
    "            store[\"A\" + str(l + 1)] = A\n",
    "            store[\"W\" + str(l + 1)] = self.parameters[\"W\" + str(l + 1)]\n",
    "            store[\"Z\" + str(l + 1)] = Z\n",
    "\n",
    "        Z = self.parameters[\"W\" + str(self.L)].dot(A) + self.parameters[\"b\" + str(self.L)]\n",
    "        A = self.sigmoid(Z)\n",
    "        store[\"A\" + str(self.L)] = A\n",
    "        store[\"W\" + str(self.L)] = self.parameters[\"W\" + str(self.L)]\n",
    "        store[\"Z\" + str(self.L)] = Z\n",
    "\n",
    "        return A, store\n",
    "\n",
    "    def sigmoid_derivative(self, Z):\n",
    "        s = 1 / (1 + np.exp(-Z))\n",
    "        return s * (1 - s)\n",
    "\n",
    "    def backward(self, X, Y, store):\n",
    "        derivatives = {}\n",
    "        store[\"A0\"] = X.T\n",
    "\n",
    "        A = store[\"A\" + str(self.L)]\n",
    "        dA = -np.divide(Y, A) + np.divide(1 - Y, 1 - A)\n",
    "\n",
    "        dZ = dA * self.sigmoid_derivative(store[\"Z\" + str(self.L)])\n",
    "        dW = dZ.dot(store[\"A\" + str(self.L - 1)].T) / self.n\n",
    "        db = np.sum(dZ, axis=1, keepdims=True) / self.n\n",
    "        dAPrev = store[\"W\" + str(self.L)].T.dot(dZ)\n",
    "\n",
    "        derivatives[\"dW\" + str(self.L)] = dW\n",
    "        derivatives[\"db\" + str(self.L)] = db\n",
    "\n",
    "        for l in range(self.L - 1, 0, -1):\n",
    "            dZ = dAPrev * self.sigmoid_derivative(store[\"Z\" + str(l)])\n",
    "            dW = 1. / self.n * dZ.dot(store[\"A\" + str(l - 1)].T)\n",
    "            db = 1. / self.n * np.sum(dZ, axis=1, keepdims=True)\n",
    "            if l > 1:\n",
    "                dAPrev = store[\"W\" + str(l)].T.dot(dZ)\n",
    "\n",
    "            derivatives[\"dW\" + str(l)] = dW\n",
    "            derivatives[\"db\" + str(l)] = db\n",
    "\n",
    "        return derivatives\n",
    "\n",
    "    def fit(self, X, Y, learning_rate=0.01, n_iterations=2500):\n",
    "        np.random.seed(1)\n",
    "        self.n = X.shape[0]\n",
    "        self.layers_size.insert(0, X.shape[1])\n",
    "        self.initialize_parameters()\n",
    "\n",
    "        for loop in range(n_iterations):\n",
    "            A, store = self.forward(X)\n",
    "            cost = np.squeeze(-(Y.dot(np.log(A.T)) + (1 - Y).dot(np.log(1 - A.T))) / self.n) # 손실함수\n",
    "            derivatives = self.backward(X, Y, store)\n",
    "\n",
    "            for l in range(1, self.L + 1):\n",
    "                self.parameters[\"W\" + str(l)] = self.parameters[\"W\" + str(l)] - learning_rate * derivatives[\n",
    "                    \"dW\" + str(l)]\n",
    "                self.parameters[\"b\" + str(l)] = self.parameters[\"b\" + str(l)] - learning_rate * derivatives[\n",
    "                    \"db\" + str(l)]\n",
    "\n",
    "            if loop % 100 == 0:\n",
    "                print(\"loop:\", loop, cost)\n",
    "                self.costs.append(cost)\n",
    "        print(\"loop:\", loop, cost)\n",
    "        self.costs.append(cost)\n",
    "\n",
    "    def predict(self, X, Y):\n",
    "        A, cache = self.forward(X)\n",
    "        n = X.shape[0]\n",
    "        p = np.zeros((1, n))\n",
    "\n",
    "        for i in range(0, A.shape[1]):\n",
    "            if A[0, i] > 0.5:\n",
    "                p[0, i] = 1\n",
    "            else:\n",
    "                p[0, i] = 0\n",
    "\n",
    "        print(\"Accuracy: \" + str(np.sum((p == Y) / n)))\n",
    "\n",
    "    def plot_cost(self):\n",
    "        plt.figure()\n",
    "        plt.plot(np.arange(len(self.costs)), self.costs)\n",
    "        plt.xlabel(\"epochs\")\n",
    "        plt.ylabel(\"cost\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def get_binary_dataset():\n",
    "    (train_x_orig, train_y_orig), (test_x_orig, test_y_orig) = mnist.load_mnist(normalize=True, flatten=True, one_hot_label=False)\n",
    "\n",
    "    index_5 = np.where(train_y_orig == 5)\n",
    "    index_8 = np.where(train_y_orig == 8)\n",
    "\n",
    "    index = np.concatenate([index_5[0], index_8[0]])\n",
    "    np.random.seed(1)\n",
    "    np.random.shuffle(index)\n",
    "\n",
    "    train_y = train_y_orig[index]\n",
    "    train_x = train_x_orig[index]\n",
    "\n",
    "    train_y[np.where(train_y == 5)] = 0\n",
    "    train_y[np.where(train_y == 8)] = 1\n",
    "\n",
    "    index_5 = np.where(test_y_orig == 5)\n",
    "    index_8 = np.where(test_y_orig == 8)\n",
    "\n",
    "    index = np.concatenate([index_5[0], index_8[0]])\n",
    "    np.random.shuffle(index)\n",
    "\n",
    "    test_y = test_y_orig[index]\n",
    "    test_x = test_x_orig[index]\n",
    "\n",
    "    test_y[np.where(test_y == 5)] = 0\n",
    "    test_y[np.where(test_y == 8)] = 1\n",
    "\n",
    "    return train_x, train_y, test_x, test_y\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_x, train_y, test_x, test_y = get_binary_dataset() # mnist 데이터셋을 불러와 Train set과 Test set으로 나눈다. 숫자 5를 Class 0, 숫자 8을 Class 1로 하여 이진분류 문제를 풀기로 한다.\n",
    "\n",
    "    print(\"train_x's shape: \" + str(train_x.shape)) # (11272, 784)\n",
    "    print(\"test_x's shape: \" + str(test_x.shape)) # (1866, 784)\n",
    "\n",
    "    layers_dims = [196, 1] # 히든 레이어의 노드는 196개\n",
    "\n",
    "    ann = ANN(layers_dims)\n",
    "    ann.fit(train_x, train_y, learning_rate=0.1, n_iterations=1000)\n",
    "    ann.predict(train_x, train_y)\n",
    "    ann.predict(test_x, test_y)\n",
    "    ann.plot_cost()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras로 살펴보는 ANN\n",
    "본격적으로 인공신경망(Artificial Neural Network) 를 공부하기에 앞서 딥러닝 라이브러리 중 하나인 Keras를 활용하여 뉴럴 네트워크의 전반적인 구조를 먼저 학습해보는 시간을 갖겠습니다.\n",
    "\n",
    "http://cs231n.github.io/assets/nn1/neural_net.jpeg\n",
    "\n",
    "뉴럴 네트워크는 여러 노드들이 수많은 layer로 구성되어 마치 사람 뇌의 뉴런처럼 데이터를 전송하고 값을 갱신해나가는 구조를 가지고 있습니다. 뉴럴 네트워크의 진행 과정은 크게 다음과 같습니다 :\n",
    "\n",
    "* 데이터 불러오기 및 가공\n",
    "* 뉴럴넷에 필요한 파라메터 세팅\n",
    "* Training\n",
    "* Test\n",
    "* 시각화\n",
    "\n",
    "구체적인 실습을 통해 뉴럴 네트워크에 좀 더 친숙해져 봅시다.\n",
    "\n",
    "### 실습\n",
    "코드의 흐름을 따라가면서 Neural Network가 어떻게 동작하는 지 살펴보세요.\n",
    "\n",
    "train_image의 shape를 출력해 보세요.\n",
    "\n",
    "train_labels의 데이터 개수를 출력해 보세요.\n",
    "\n",
    "train_labels를 출력해 보세요.\n",
    "\n",
    "test_image의 shape를 출력해 보세요.\n",
    "\n",
    "test_labels의 데이터 개수를 출력해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib.pyplot'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b05f2e557d6f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib.pyplot'"
     ]
    }
   ],
   "source": [
    "\n",
    "# tensorflow와 tf.keras를 임포트합니다\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def main():\n",
    "\n",
    "    # Fashion mnist data 를 load 합니다.\n",
    "    fashion_mnist = keras.datasets.fashion_mnist\n",
    "    (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "    train_images = train_images / 255.0\n",
    "    test_images = test_images / 255.0\n",
    "\n",
    "    # label에 해당하는 class name 입니다.\n",
    "    class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "                   'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "\n",
    "    # 2-layer 모델을 생성합니다.\n",
    "    model = keras.Sequential([\n",
    "        # x data가 2차원인 것을 1차원으로 변경해줍니다.\n",
    "        keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        # 첫번째 layer의 뉴런 개수를 정합니다.\n",
    "        keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "        # 두번째 layer의 뉴런 개수를 정합니다. (output)\n",
    "        keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "    ])\n",
    "\n",
    "    # 모델의 상세 기법을 정의합니다.\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # 모델을 train image, train label을 가지고 학습시킵니다.\n",
    "    model.fit(train_images, train_labels, epochs=5)\n",
    "\n",
    "\n",
    "    # 모델의 loss와 accuracy를 계산합니다.\n",
    "    test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "\n",
    "    print('\\n- TEST 정확도 :', test_acc)\n",
    "\n",
    "\n",
    "    # 학습한 모델로 테스트 데이터를 예측합니다.\n",
    "    predictions = model.predict(test_images)\n",
    "\n",
    "\n",
    "    # 2. train_image의 shape를 출력해 보세요.\n",
    "    print(\"\\n- train image shape : \\n\",train_images.shape)\n",
    "    \n",
    "    # 3. train_labels의 데이터 개수를 출력해 보세요.\n",
    "    print(\"\\n- train labels length : \\n\", len(train_labels))\n",
    "    \n",
    "    # 4. train_labels를 출력해 보세요.\n",
    "    print(\"\\n- train labels : \\n\", train_labels)\n",
    "    \n",
    "    # 5. test_image의 shape를 출력해 보세요.\n",
    "    print(\"\\n- test image shape : \\n\", test_images.shape)\n",
    "    \n",
    "    # 6. test_labels의 데이터 개수를 출력해 보세요.\n",
    "    print(\"\\n- test labels length : \\n\", len(test_labels))\n",
    "    print()\n",
    "    \n",
    "\n",
    "    # test_images의 0번째 data를 제대로 맞췄는지 확인해봅니다.\n",
    "    i = 0\n",
    "    plt.figure(figsize=(6,3))\n",
    "    plt.subplot(1,2,1)\n",
    "    plot_image(i, predictions, test_labels, test_images, class_names)\n",
    "    plt.subplot(1,2,2)\n",
    "    plot_value_array(i, predictions,  test_labels)\n",
    "#     plt.savefig('ANN.png')\n",
    "\n",
    "    # test_images의 12번째 data를 제대로 맞췄는지 확인해봅니다.\n",
    "    i = 12\n",
    "    plt.figure(figsize=(6,3))\n",
    "    plt.subplot(1,2,1)\n",
    "    plot_image(i, predictions, test_labels, test_images, class_names)\n",
    "    plt.subplot(1,2,2)\n",
    "    plot_value_array(i, predictions,  test_labels)\n",
    "#     plt.savefig('ANN.png')\n",
    "\n",
    "\n",
    "def plot_image(i, predictions_array, true_label, img, class_names):\n",
    "    predictions_array, true_label, img = predictions_array[i], true_label[i], img[i]\n",
    "    plt.grid(False)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.imshow(img, cmap=plt.cm.binary)\n",
    "\n",
    "    predicted_label = np.argmax(predictions_array)\n",
    "    if predicted_label == true_label:\n",
    "        color = 'blue'\n",
    "    else:\n",
    "        color = 'red'\n",
    "\n",
    "    plt.xlabel(\"{} {:2.0f}% (true:{})\".format(class_names[predicted_label],\n",
    "                                    100*np.max(predictions_array),\n",
    "                                    class_names[true_label]),\n",
    "                                    color=color)\n",
    "\n",
    "def plot_value_array(i, predictions_array, true_label):\n",
    "    predictions_array, true_label = predictions_array[i], true_label[i]\n",
    "    plt.grid(False)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n",
    "    plt.ylim([0, 1])\n",
    "    predicted_label = np.argmax(predictions_array)\n",
    "\n",
    "    thisplot[predicted_label].set_color('red')\n",
    "    thisplot[true_label].set_color('blue')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 인공신경망 모델 설계하기\n",
    "하나의 Layer에 대한 연산은 각각의 입력 값(input)과 가중치(weight) 값들의 곱과 전체의 합, 그리고 activation function을 통과하여 비선형의 결과값으로 나타납니다.\n",
    "\n",
    "http://cs231n.github.io/assets/nn1/neuron_model.jpeg\n",
    "    \n",
    "이렇게 모델링을 하는 이유는, 입력값 (X, y)(X,y) 가 주어져 있을 때, 복잡한 문제를 선형 함수 y = WX + by=WX+b 와 비선형 함수 (sigmoid function, etc) 로 모델링을 하여, 실제 원하는 가중치(WW)를 뽑아내기 위함입니다.\n",
    "\n",
    "원래는 어떠한 현상을 모델링 하려면 사람이 수학적으로 복잡한 함수를 미리 선정하고, 현재 가지고 있는 입력값을 대입해서, 그에 맞는 가중치(WW)를 판단해야 하는데요. 입력값에 맞는 함수를 선정하는 것은 비용이 많이 들고, 매우 어려운 일입니다.\n",
    "\n",
    "인공신경망에서 선형 함수와 비선형 함수의 간단한 조합으로 사람이 Layer 층의 수, Layer 당 Node 개수, 학습 속도 등등의 파라미터만 조정해준다면,\n",
    "\n",
    "컴퓨터가 알아서 최적화 된 가중치(WW)를 판단해주고, 이로써 정확도가 굉장히 높은 모델을 만들어 낼 수 있습니다.\n",
    "\n",
    "이번 실습에서는 [실습 13-1]과 마찬가지로 Fashion Mnist data set을 가지고 직접 2층 짜리 뉴럴 네트워크 모델을 구축해 보겠습니다.\n",
    "\n",
    "### 실습\n",
    "Line 101 의 neural_net() 함수를 채워가며 직접 모델을 설계해 보겠습니다.\n",
    "\n",
    "layer을 완성하려면 다음과 같은 절차가 필요합니다.\n",
    "\n",
    "* 입력인자 x, weights, biases가 Line 58, 63의 dictionary 형태이니, 여기서 값을 하나씩 추출해서 사용하세요.\n",
    "* y = WX+by=WX+b 의 수식으로 하나의 layer가 완성됩니다.\n",
    "* 여기에 sigmoid function을 씌워 비선형으로 만들어 줍니다.\n",
    "\n",
    "#### 아래 함수들을 이용해서 layer1, layer2, out layer, 총 3개의 layer를 완성해 보세요.\n",
    "\n",
    "(out layer는 sigmoid function을 씌우지 않습니다. 왜냐하면 마지막에 softmax function을 적용하기 때문입니다.)\n",
    "\n",
    "#### 사용 될 tensorflow 함수\n",
    "\n",
    "* tf.add(a,b) : a와 b를 더합니다.\n",
    "* tf.matmul(a,b) : a와 b의 행렬곱을 합니다.\n",
    "* tf.nn.sigmoid(a) : a에 sigmoid 연산을 적용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    # Fasion mnist data를 불러옵니다.\n",
    "    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "    \n",
    "    \n",
    "    # y에 해당하는 전체 클래스 개수를 의미합니다.\n",
    "    num_classes = 10\n",
    "    \n",
    "    # y에 해당하는 class name 입니다.\n",
    "    class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat','Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "    \n",
    "    # image shape : 28*28 = data features\n",
    "    num_features = 784\n",
    "\n",
    "    # 1st layer 뉴런의 개수를 설정합니다.\n",
    "    hidden_layer_1 = 64\n",
    "    \n",
    "    # 2nd layer 뉴런의 개수를 설정합니다.\n",
    "    hidden_layer_2 = 128\n",
    "\n",
    "    # Training 을 위한 파라메터입니다.\n",
    "    learning_rate = 0.05\n",
    "    training_steps = 3000\n",
    "    batch_size = 512\n",
    "    interval = 300\n",
    "\n",
    "\n",
    "    # Training data를 float32 자료형으로 형변환 해줍니다.\n",
    "    x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)\n",
    "    \n",
    "    # 2차원 이미지를 1차원으로 변형합니다.\n",
    "    x_train, x_test = x_train.reshape([-1, num_features]), x_test.reshape([-1, num_features])\n",
    "    \n",
    "    # 값을 [0, 255] 에서 [0, 1]으로 정규화 해줍니다.\n",
    "    x_train, x_test = x_train / 255., x_test / 255.\n",
    "\n",
    "    # training data를 무작위로 선정하고, batch_size만큼 나눠주면서 초기화를 진행합니다.\n",
    "    train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    train_data = train_data.repeat().shuffle(10000).batch(batch_size).prefetch(1)\n",
    "\n",
    "\n",
    "    # weight 값들을 무작위로 초기화합니다.\n",
    "    random_normal = tf.initializers.RandomNormal()\n",
    "\n",
    "    # 각 layer에 들어갈 Weight, bias들을 할당해 줍니다.\n",
    "    weights = {\n",
    "        'h1': tf.Variable(random_normal([num_features, hidden_layer_1])),\n",
    "        'h2': tf.Variable(random_normal([hidden_layer_1, hidden_layer_2])),\n",
    "        'out': tf.Variable(random_normal([hidden_layer_2, num_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.zeros([hidden_layer_1])),\n",
    "        'b2': tf.Variable(tf.zeros([hidden_layer_2])),\n",
    "        'out': tf.Variable(tf.zeros([num_classes]))\n",
    "    }\n",
    "\n",
    "\n",
    "    # Training을 실제로 구현하는 부분입니다.\n",
    "    for step, (batch_x, batch_y) in enumerate(train_data.take(training_steps), 1):\n",
    "        # W 와 b 파라메터를 갱신하기 위해 함수를 호출합니다.\n",
    "        params_optimization(batch_x, batch_y, weights, biases, num_classes, learning_rate)\n",
    "\n",
    "        if step % interval == 0:\n",
    "            '''\n",
    "            neural_net() 함수는 model 생성에 사용되는 함수입니다.\n",
    "            neural_net() 함수를 채워 pred를 완성해보도록 하겠습니다.\n",
    "            '''\n",
    "            pred = neural_net(batch_x, weights, biases)\n",
    "            \n",
    "            loss = cross_entropy(pred, batch_y, num_classes)\n",
    "            acc = accuracy(pred, batch_y)\n",
    "            print(\"step: %i, loss: %f, accuracy: %f\" % (step, loss, acc))\n",
    "\n",
    "\n",
    "    # Test를 진행하여 실제 정확도를 확인합니다.\n",
    "    pred = neural_net(x_test, weights, biases)\n",
    "    print(\"Test Accuracy: %f\" % accuracy(pred, y_test))\n",
    "    \n",
    "    \n",
    "    # 결과를 시각화합니다.\n",
    "    plot_neural_network_results(pred, x_test, y_test, class_names)\n",
    "\n",
    "\n",
    "    \n",
    "# 인공 신경망 함수입니다.\n",
    "'''\n",
    "def neural_net(x, weights, biases) 함수를 채워보세요.\n",
    "'''\n",
    "def neural_net(x, weights, biases):\n",
    "    \n",
    "    # 총 2개의 layer로 이뤄져 있으며, fully connected 연산 후 sigmoid 함수에 적용하여 값을 비선형으로 만들어줍니다.\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.sigmoid(layer_1)\n",
    "    \n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.sigmoid(layer_2)\n",
    "    \n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    \n",
    "    return tf.nn.softmax(out_layer)\n",
    "\n",
    "\n",
    "# Cross-Entropy loss function에 대한 함수입니다.\n",
    "def cross_entropy(y_pred, y_true, num_classes):\n",
    "\n",
    "    y_true = tf.one_hot(y_true, depth=num_classes)\n",
    "    \n",
    "    # 값 중에 min 값이 1e-8, max 값이 1.0 이 넘어가는 값이 없도록 바운더리를 지정합니다.\n",
    "    y_pred = tf.clip_by_value(y_pred, 1e-8, 1.)\n",
    "    \n",
    "    # cross-entropy를 수식으로 입력해 보세요.\n",
    "    return tf.reduce_mean(-tf.reduce_sum(y_true * tf.math.log(y_pred)))\n",
    "\n",
    "\n",
    "# 정확도(accuracy) 를 측정하는 함수입니다.\n",
    "def accuracy(y_pred, y_true):\n",
    "    # Predicted class is the index of highest score in prediction vector (i.e. argmax).\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n",
    "    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32), axis=-1)\n",
    "\n",
    "\n",
    "# 파라메터들을 갱신하는 함수입니다. \n",
    "def params_optimization(x, y, weights, biases, num_classes, learning_rate):\n",
    "    # GradientTape() 함수를 묶어서 gradient를 자동 계산하도록 돕습니다.\n",
    "    with tf.GradientTape() as gt:\n",
    "        pred = neural_net(x, weights, biases)\n",
    "        loss = cross_entropy(pred, y, num_classes)\n",
    "        \n",
    "    # variable을 업데이트합니다.\n",
    "    variables_to_update = list(weights.values()) + list(biases.values())\n",
    "\n",
    "    # gradient를 계산합니다.\n",
    "    gradients = gt.gradient(loss, variables_to_update)\n",
    "    \n",
    "    # 파라메터(weight)들을 갱신하기 위한 함수를 지정해줍니다.\n",
    "    optimizer = tf.optimizers.Adagrad(learning_rate)\n",
    "    \n",
    "    # optimizer로 W와 b를 갱신합니다.\n",
    "    optimizer.apply_gradients(zip(gradients, variables_to_update))\n",
    "    \n",
    "\n",
    "def plot_neural_network_results(pred, x_test, y_test, class_names):\n",
    "    # 결과를 시각화 하는 함수입니다.\n",
    "    n_images = 10\n",
    "    test_images = x_test[:n_images]\n",
    "    predictions = pred\n",
    "\n",
    "    plt.figure(figsize=(7,10))\n",
    "    for i in range(n_images):\n",
    "        plt.subplot(5,2,i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        plt.imshow(np.reshape(test_images[i], [28, 28]), cmap=plt.cm.binary)\n",
    "        plt.xlabel(\"True : {} (Predict : {})\".format(class_names[y_test[i]],\n",
    "                                    class_names[np.argmax(predictions.numpy()[i])]))\n",
    "#     plt.savefig('ANN.png')\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function 구현해보기\n",
    "분류 문제에서 정답 y_{true}ytrue 와 예측값 y_{pred}ypred 사이에 오차를 판별하기 위해서는 어떤 Loss function을 사용할까요?\n",
    "\n",
    "회귀 문제에서는 Mean Squared Error가 오차 판별에 적합했다면, 분류 문제에서는 Cross entropy 함수가 오차를 판별하는 loss function으로 사용되고 있습니다.\n",
    "\n",
    "(링크 참조 - 분류 오차에 Cross entropy를 사용하는 이유)\n",
    "\n",
    "http://funmv2013.blogspot.com/2017/01/cross-entropy.html\n",
    "\n",
    "H_{p, q}(X) = - \\sum_{i=1}^{N} p(x_{i}) log q(x_{i}) H p,q\t (X)=− i=1∑N p(x i )logq(x i )\n",
    "\n",
    "Cross entropy는 두 개의 확률 분포 p, qp,q에 대해 하나의 사건 XX 가 갖는 정보량 입니다. 다시 말해서, 서로 다른 두 확률 분포에 대해 같은 사건이 가지는 정보량을 계산한 것입니다. 이는 qq 에 대한 정보량을 pp 에 대해서 평균을 낸 것으로 볼 수 있습니다.\n",
    "\n",
    "구체적인 실습을 통해 Cross entropy에 좀 더 친숙해져 봅시다.\n",
    "\n",
    "### 실습\n",
    "* Line 101의 neural_net()함수를 완성시켜보세요.\n",
    "\n",
    "* Line 120 의 cross_entropy() 함수를 채워가며 직접 loss function을 구현해 보겠습니다.\n",
    "\n",
    "Cross entropy loss function 은 다음과 같습니다.\n",
    "\n",
    "H(y_{true}, y_{pred}) = - \\frac {1}{N} \\sum_{i=1}^{N} y_{true_i} log(y_{pred_i}) H(y true ,y pred )=− N1i=1∑N y true i log(y pred i )\n",
    "\n",
    "아래 함수들을 이용해서 cross entropy 를 완성해 보세요.\n",
    "\n",
    "### 사용 될 tensorflow 함수\n",
    "\n",
    "* tf.reduce_mean(x) : x값에 대해 전체 평균을 구합니다.\n",
    "* tf.reduce_sum(x) : x값에 대해 전체 덧셈을 합니다.\n",
    "* tf.math.log(x) : x에 log 연산을 적용합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-bfd989ccb077>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfashion_mnist\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.keras'"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    # Fasion mnist data를 불러옵니다.\n",
    "    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "    \n",
    "    \n",
    "    # y에 해당하는 전체 클래스 개수를 의미합니다.\n",
    "    num_classes = 10\n",
    "    \n",
    "    # y에 해당하는 class name 입니다.\n",
    "    class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat','Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "    \n",
    "    # image shape : 28*28 = data features\n",
    "    num_features = 784\n",
    "\n",
    "    # 1st layer 뉴런의 개수를 설정합니다.\n",
    "    hidden_layer_1 = 64\n",
    "    \n",
    "    # 2nd layer 뉴런의 개수를 설정합니다.\n",
    "    hidden_layer_2 = 128\n",
    "\n",
    "    # Training 을 위한 파라메터입니다.\n",
    "    learning_rate = 0.05\n",
    "    training_steps = 3000\n",
    "    batch_size = 512\n",
    "    interval = 300\n",
    "\n",
    "\n",
    "    # Training data를 float32 자료형으로 형변환 해줍니다.\n",
    "    x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)\n",
    "    \n",
    "    # 2차원 이미지를 1차원으로 변형합니다.\n",
    "    x_train, x_test = x_train.reshape([-1, num_features]), x_test.reshape([-1, num_features])\n",
    "    \n",
    "    # 값을 [0, 255] 에서 [0, 1]으로 정규화 해줍니다.\n",
    "    x_train, x_test = x_train / 255., x_test / 255.\n",
    "\n",
    "    # training data를 무작위로 선정하고, batch_size만큼 나눠주면서 초기화를 진행합니다.\n",
    "    train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    train_data = train_data.repeat().shuffle(10000).batch(batch_size).prefetch(1)\n",
    "\n",
    "\n",
    "    # weight 값들을 무작위로 초기화합니다.\n",
    "    random_normal = tf.initializers.RandomNormal()\n",
    "\n",
    "    # 각 layer에 들어갈 Weight, bias들을 할당해 줍니다.\n",
    "    weights = {\n",
    "        'h1': tf.Variable(random_normal([num_features, hidden_layer_1])),\n",
    "        'h2': tf.Variable(random_normal([hidden_layer_1, hidden_layer_2])),\n",
    "        'out': tf.Variable(random_normal([hidden_layer_2, num_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.zeros([hidden_layer_1])),\n",
    "        'b2': tf.Variable(tf.zeros([hidden_layer_2])),\n",
    "        'out': tf.Variable(tf.zeros([num_classes]))\n",
    "    }\n",
    "\n",
    "\n",
    "    # Training을 실제로 구현하는 부분입니다.\n",
    "    for step, (batch_x, batch_y) in enumerate(train_data.take(training_steps), 1):\n",
    "        # W 와 b 파라메터를 갱신하기 위해 함수를 호출합니다.\n",
    "        params_optimization(batch_x, batch_y, weights, biases, num_classes, learning_rate)\n",
    "\n",
    "        if step % interval == 0:\n",
    "            pred = neural_net(batch_x, weights, biases)\n",
    "            '''\n",
    "            cross_entropy() 함수는 loss function입니다.\n",
    "            cross_entropy() 함수를 채워 loss 값을 완성해보도록 하겠습니다.\n",
    "            '''\n",
    "            loss = cross_entropy(pred, batch_y, num_classes)\n",
    "            \n",
    "            acc = accuracy(pred, batch_y)\n",
    "            print(\"step: %i, loss: %f, accuracy: %f\" % (step, loss, acc))\n",
    "\n",
    "\n",
    "    # Test를 진행하여 실제 정확도를 확인합니다.\n",
    "    pred = neural_net(x_test, weights, biases)\n",
    "    print(\"Test Accuracy: %f\" % accuracy(pred, y_test))\n",
    "    \n",
    "    \n",
    "    # 결과를 시각화합니다.\n",
    "    plot_neural_network_results(pred, x_test, y_test, class_names)\n",
    "\n",
    "\n",
    "    \n",
    "# 인공 신경망 함수입니다.\n",
    "'''\n",
    "def neural_net(x, weights, biases) 함수를 채워보세요.\n",
    "'''\n",
    "def neural_net(x, weights, biases):\n",
    "    \n",
    "    # 총 2개의 layer로 이뤄져 있으며, fully connected 연산 후 sigmoid 함수에 적용하여 값을 비선형으로 만들어줍니다.\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.sigmoid(layer_1)\n",
    "    \n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.sigmoid(layer_2)\n",
    "    \n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    \n",
    "    return tf.nn.softmax(out_layer)\n",
    "\n",
    "\n",
    "# Cross-Entropy loss function에 대한 함수입니다.\n",
    "'''\n",
    "def cross_entropy(y_pred, y_true, num_classes) 함수를 채워보세요.\n",
    "'''\n",
    "def cross_entropy(y_pred, y_true, num_classes):\n",
    "\n",
    "    y_true = tf.one_hot(y_true, depth=num_classes)\n",
    "    \n",
    "    # 값 중에 min 값이 1e-8, max 값이 1.0 이 넘어가는 값이 없도록 바운더리를 지정합니다.\n",
    "    y_pred = tf.clip_by_value(y_pred, 1e-8, 1.)\n",
    "    N = y_pred.shape[0]\n",
    "    # cross-entropy를 수식으로 입력해 보세요.\n",
    "    return -tf.reduce_mean((tf.reduce_sum(y_true * tf.math.log(y_pred))))\n",
    "\n",
    "# 정확도(accuracy) 를 측정하는 함수입니다.\n",
    "def accuracy(y_pred, y_true):\n",
    "    # Predicted class is the index of highest score in prediction vector (i.e. argmax).\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n",
    "    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32), axis=-1)\n",
    "\n",
    "\n",
    "# 파라메터들을 갱신하는 함수입니다. \n",
    "def params_optimization(x, y, weights, biases, num_classes, learning_rate):\n",
    "    # GradientTape() 함수를 묶어서 gradient를 자동 계산하도록 돕습니다.\n",
    "    with tf.GradientTape() as gt:\n",
    "        pred = neural_net(x, weights, biases)\n",
    "        loss = cross_entropy(pred, y, num_classes)\n",
    "        \n",
    "    # variable을 업데이트합니다.\n",
    "    variables_to_update = list(weights.values()) + list(biases.values())\n",
    "\n",
    "    # gradient를 계산합니다.\n",
    "    gradients = gt.gradient(loss, variables_to_update)\n",
    "    \n",
    "    # 파라메터(weight)들을 갱신하기 위한 함수를 지정해줍니다.\n",
    "    optimizer = tf.optimizers.Adagrad(learning_rate)\n",
    "    \n",
    "    # optimizer로 W와 b를 갱신합니다.\n",
    "    optimizer.apply_gradients(zip(gradients, variables_to_update))\n",
    "    \n",
    "\n",
    "def plot_neural_network_results(pred, x_test, y_test, class_names):\n",
    "    # 결과를 시각화 하는 함수입니다.\n",
    "    n_images = 10\n",
    "    test_images = x_test[:n_images]\n",
    "    predictions = pred\n",
    "\n",
    "    plt.figure(figsize=(7,10))\n",
    "    for i in range(n_images):\n",
    "        plt.subplot(5,2,i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        plt.imshow(np.reshape(test_images[i], [28, 28]), cmap=plt.cm.binary)\n",
    "        plt.xlabel(\"True : {} (Predict : {})\".format(class_names[y_test[i]],\n",
    "                                    class_names[np.argmax(predictions.numpy()[i])]))\n",
    "#     plt.savefig('ANN.png')\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back propagation 개념 학습\n",
    "인공신경망은 수많은 노드와 가중치(weight)들을 합하고, 비선형 함수를 계산하여 나타내는 모델입니다. 이 때, loss function에 모델을 적용했을 때 loss function 자체가 convex 하지 않게 되어 global minimum 값을 찾을 수가 없습니다.\n",
    "\n",
    "https://static.thinkingandcomputing.com/2014/03/bprop.png\n",
    "\n",
    "non-convex한 함수에 대한 minimum을 찾기 위해, 점진적으로 하강하면서 근방에 local minimum에 접근하는 방법이 있는데요. 이 때 활용되는 것이 gradient descent와 같은 방법입니다.\n",
    "\n",
    "Back propagation은 loss function에서 각 가중치(weights)에 대해 미분을 진행하여, 그 미분값의 반대방향으로 gradient descent를 하기 위해 사용되는 기법입니다. 가중치의 미분값은 결과인 loss function에 대한 기울기이자, 1만큼 증가할 때의 변화량을 의미합니다.\n",
    "\n",
    "Back propagation을 위해서 Chain rule 방법으로 복잡한 미분을 간단하게 계산합니다.\n",
    "\n",
    "-------------------------\n",
    "Chain rule 공식\n",
    "\n",
    "https://www.onlinemathlearning.com/image-files/chain-rule.png\n",
    "\n",
    "구체적인 실습을 통해 Back propagation 에 대해 알아봅시다.\n",
    "\n",
    "### 실습\n",
    "Chain rule 을 활용하여 직접\n",
    "\n",
    "f(x,y,z) = (x + y) * zf(x,y,z)=(x+y)∗z\n",
    "\n",
    "에 대한 x, y, zx,y,z 의 미분값을 구해봅시다.\n",
    "\n",
    "\n",
    "df/dx\n",
    "​\t , \n",
    "df/dy\n",
    "​\t , \n",
    "df/dz\n",
    "​\t \n",
    "\n",
    "원래 함수를 다음 함수로 잘게 쪼개어 각 미분값을 구해보세요.\n",
    "\n",
    "f = h * zf=h∗z\n",
    "h = x + yh=x+y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 5 -4\n",
      "답 : 정답입니다.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "\t# Input 값을 다음과 같이 할당합니다.\n",
    "    x = -7; y = 3; z = 5\n",
    "\n",
    "    # Forward propatation 은 차례로 모델의 값을 입력하는 방식입니다.\n",
    "    # f = (x + y) * z 를 다음과 같이 각 재정의를 통해서 선언합니다.\n",
    "    h = x + y\n",
    "    f = h * z\n",
    "\n",
    "    # 거꾸로 Back propagation을 진행해봅시다:\n",
    "    # 첫번째 미분 대상은 f = h * z 입니다.\n",
    "    dfdh = z\n",
    "    dfdz = h\n",
    "\n",
    "    # 다음 미분 대상은 h = x + y 입니다.\n",
    "    dhdx = 1\n",
    "    dhdy = 1\n",
    "\n",
    "    # Chain rule 을 적용하여 값을 구해보세요.\n",
    "    dfdx = dfdh * dhdx\n",
    "    dfdy = dfdh * dhdy\n",
    "    \n",
    "    print (dfdx,dfdy,dfdz)\n",
    "\n",
    "    if(dfdx == 5 and dfdy == 5 and dfdz == -4):\n",
    "        print(\"답 : 정답입니다.\")\n",
    "    else:\n",
    "        print(\"답 : 입력값을 다시 넣어 보세요.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigmoid back propagation 연산 구현\n",
    "Sigmoid 함수에 x값을 입력하면, 결과 값이 0과 1사이로 리턴 됩니다. 이는 인공신경망에서 선형 결합인 WX + bWX+b 값을 0과 1사이로 변환하여 비선형으로 만들어서 분류 문제를 쉽게 풀 수 있도록 도와줍니다.\n",
    "\n",
    "https://miro.medium.com/max/1838/1*JHWL_71qml0kP_Imyx4zBg.png\n",
    "\n",
    "Forward propagation으로 연산이 되면, loss function의 local minimum을 찾기 위해 back propagation이 진행되면서 sigmoid 함수 또한 입력 값으로 미분됩니다.\n",
    "\n",
    "이번 미션에서는 sigmoid 함수를 지나는 w, x값에 대하여 chain rule을 적용하여 미분 값을 구해보고, back propagation 연산을 진행해보도록 하겠습니다.\n",
    "\n",
    "### Sigmoid 함수\n",
    "\n",
    "f(w, x) = \\frac{1}/{1 + e^{-(w_0 * x_0 + w_1 * x_1 + w_2)}}\n",
    "\n",
    "### 실습\n",
    "Chain rule 을 활용하여 직접\n",
    "\n",
    "f(w, x) = \\frac{1} / {1 + e^{-(w_0 * x_0 + w_1 * x_1 + w_2)}}\n",
    "\n",
    "에 대한 w_0, x_0, w_1, x_1, w_2w   의 미분값을 구해봅시다.\n",
    "\n",
    "\\frac{df}/{dw_0}, \\frac{df}/{dx_0}, \\frac{df}/{dw_1}, \\frac{df}/{dx_1}, \\frac{df}/{dw_2} \n",
    "\n",
    "원래 함수를 다음 함수로 잘게 쪼개어 각 미분값을 코드 상에 입력해보세요.\n",
    "\n",
    "f = \\frac{1}/{g}\n",
    "\n",
    "g = 1 + e^{h}\n",
    " \n",
    "h = -(w_0 * x_0 + w_1 * x_1 + w_2)\n",
    "\n",
    "각 함수의 미분 값은 다음과 같습니다.\n",
    "\n",
    "\\frac{df}/{dg} = \\frac{-1}/{g^2} \n",
    "\n",
    "\\frac{dg}/{dh} = e^h \n",
    "\n",
    "\\frac{dh}/{dw_0} = -x_0 \n",
    "\n",
    "\\frac{dh}/{dx_0} = -w_0 \n",
    "\n",
    "\\frac{dh}/{dw_1} = -x_1 \n",
    "\n",
    "\\frac{dh}/{dx_1} = -w_x \n",
    "\n",
    "\\frac{dh}/{dw_2} = -1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.19661193324148188 0.39322386648296376 -0.39322386648296376 -0.5898357997244457 0.19661193324148188\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def main():\n",
    "    # Input 값을 다음과 같이 할당합니다.\n",
    "    w_0 = 2; x_0 = -1; w_1 = -3; x_1 = -2; w_2 = -3\n",
    "\n",
    "    # Forward propatation 은 차례로 모델의 값을 입력하는 방식입니다.\n",
    "    # f(w, x) 를 다음과 같이 각 재정의를 통해서 선언합니다.\n",
    "    h = -((w_0*x_0)+(w_1*x_1)+w_2)\n",
    "    g = 1+np.exp(h)\n",
    "    f = 1/g\n",
    "\n",
    "    # 거꾸로 Back propagation을 진행해봅시다:\n",
    "    # 첫번째 미분 대상은 f = 1 / g 입니다.\n",
    "    dfdg = -1/(g**2)\n",
    "\n",
    "    # 다음 미분 대상은 g = 1 + exp(h) 입니다.\n",
    "    dgdh = np.exp(h)\n",
    "    \n",
    "    # 세번째 미분 대상은 h = -(w_0 * x_0 + w_1 * x_1 + w_2) 입니다.\n",
    "    dhdw_0 = -x_0\n",
    "    dhdx_0 = -w_0\n",
    "    dhdw_1 = -x_1\n",
    "    dhdx_1 = -w_1\n",
    "    dhdw_2 = -1\n",
    "\n",
    "    # Chain rule 을 적용하여 값을 구해보세요.\n",
    "    dfdw_0 = dfdg*dgdh*dhdw_0\n",
    "    dfdx_0 = dfdg*dgdh*dhdx_0\n",
    "    dfdw_1 = dfdg*dgdh*dhdw_1\n",
    "    dfdx_1 = dfdg*dgdh*dhdx_1\n",
    "    dfdw_2 = dfdg*dgdh*dhdw_2\n",
    "    print (dfdw_0,dfdx_0,dfdw_1,dfdx_1,dfdw_2)\n",
    "    return dfdw_0, dfdx_0, dfdw_1, dfdx_1, dfdw_2\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
